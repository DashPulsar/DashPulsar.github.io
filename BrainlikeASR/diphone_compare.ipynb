{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "\n",
    "def CTC_index(processor,outind):\n",
    "    meaningful_ids = []\n",
    "    meaningful_indices = []\n",
    "    previous_id = -1  \n",
    "    blank_token_id = processor.tokenizer.pad_token_id  \n",
    "    for i, token_id in enumerate(outind[0]):  \n",
    "        if token_id != previous_id and token_id != blank_token_id:\n",
    "            meaningful_ids.append(token_id.item())  \n",
    "            meaningful_indices.append(i)  \n",
    "        previous_id = token_id\n",
    "    \n",
    "    return meaningful_indices\n",
    "\n",
    "def get_set_diphone(paths,model,processor):\n",
    "    out_dict={}\n",
    "    for each_sentence in tqdm.tqdm(paths):\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "\n",
    "        sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        input=processor(wave_res,sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "        input=input.to(device)\n",
    "        model.to(device)\n",
    "        with torch.no_grad():\n",
    "            out_encoder1=model(input).logits\n",
    "        outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "        transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "        for i in range(len(transcription)-1):\n",
    "            key = transcription[i] + transcription[i + 1]\n",
    "            if key not in out_dict:\n",
    "                out_dict[key] = []\n",
    "            out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    "    return out_dict\n",
    "\n",
    "ALL_ENG_ENG_path=r\"..\\data\\raw_L1\"\n",
    "ALL_ENG_ENG_pathset=get_pathset(ALL_ENG_ENG_path)\n",
    "ALL_ENG_ENG_dict = get_set_diphone(ALL_ENG_ENG_pathset, model_P, processor_P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get sentence and keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_path=r\"..\\data\\test.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A boy fell from the window.',\n",
       " 'Big dogs can be dangerous.',\n",
       " 'He grew lots of vegetables.',\n",
       " 'She argues with her sister.',\n",
       " \"She's drinking from her own cup.\",\n",
       " 'Somebody stole the money.',\n",
       " 'The bananas are too ripe.',\n",
       " 'The car is going too fast.',\n",
       " 'The family likes fish.',\n",
       " 'The fire was very hot.',\n",
       " 'The kitchen window was clean.',\n",
       " 'The paint dripped on the ground.',\n",
       " 'The picture came from a book.',\n",
       " 'The player lost a shoe.',\n",
       " 'The shoes were very dirty.',\n",
       " 'The wife helped her husband.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_result_1a=human_result[human_result[\"Experiment\"]==\"1a\"]\n",
    "human_result_1a_set21=human_result_1a[human_result_1a[\"TrainingTestSet\"]==\"set2,set1\"]\n",
    "\n",
    "human_result_1a_set12=human_result_1a[human_result_1a[\"TrainingTestSet\"]==\"set1,set2\"]\n",
    "set(human_result_1a_set21[\"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentenceID_1a=set([int(i[-2:])-1 for i in list(set(human_result_1a_set21[\"SentenceID\"]))])\n",
    "sentenceID_1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boy', ' fell', ' window']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(human_result_1a_set21[human_result_1a_set21[\"SentenceID\"]=='HT1_S001'][\"Keywords\"])[0].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HT1_S001',\n",
       " 'HT1_S002',\n",
       " 'HT1_S003',\n",
       " 'HT1_S004',\n",
       " 'HT1_S005',\n",
       " 'HT1_S006',\n",
       " 'HT1_S007',\n",
       " 'HT1_S008',\n",
       " 'HT1_S009',\n",
       " 'HT1_S010',\n",
       " 'HT1_S011',\n",
       " 'HT1_S013',\n",
       " 'HT1_S014',\n",
       " 'HT1_S015',\n",
       " 'HT1_S016',\n",
       " 'HT1_S017']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(set(human_result_1a_set21[\"SentenceID\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boy', ' fell', ' window']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(human_result_1a_set21[human_result_1a_set21[\"SentenceID\"]==\"HT1_S001\"][\"Keywords\"])[0].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "set21_keywords_list={str(int(each_ID[-2:])-1):list(human_result_1a_set21[human_result_1a_set21[\"SentenceID\"]==each_ID][\"Keywords\"])[0].split(\",\") for each_ID in sorted(list(set(human_result_1a_set21[\"SentenceID\"])))}\n",
    "set21_keywords_list={key:[value.strip() for value in values] for key,values in set21_keywords_list.items()}\n",
    "\n",
    "set12_keywords_list={str(int(each_ID[-2:])-1):list(human_result_1a_set12[human_result_1a_set12[\"SentenceID\"]==each_ID][\"Keywords\"])[0].split(\",\") for each_ID in sorted(list(set(human_result_1a_set12[\"SentenceID\"])))}\n",
    "set12_keywords_list={key:[value.strip() for value in values] for key,values in set12_keywords_list.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'raincoat'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set12_keywords_list[\"17\"][-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('17', ['hung', 'raincoat']),\n",
       " ('18', ['mailman', 'brought', 'letter']),\n",
       " ('19', ['mother', 'heard', 'baby']),\n",
       " ('20', ['found', 'purse', 'trash']),\n",
       " ('21', ['table', 'has', 'three', 'legs']),\n",
       " ('22', ['children', 'waved', 'train']),\n",
       " ('24', ['girl', 'fixing', 'dress']),\n",
       " ('25', ['time', 'go', 'bed']),\n",
       " ('26', ['mother', 'read', 'instructions']),\n",
       " ('27', ['dog', 'eating', 'some', 'meat']),\n",
       " ('28', ['father', 'forgot', 'bread']),\n",
       " ('29', ['road', 'goes', 'hill']),\n",
       " ('30', ['painter', 'uses', 'brush']),\n",
       " ('31', ['family', 'bought', 'house']),\n",
       " ('37', ['had', 'two', 'empty', 'bottles']),\n",
       " ('40', ['house', 'had', 'nine', 'bedrooms'])]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set12_keywords_list.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interval(32.049, 33.518, HE HUNG UP HIS RAINCOAT),\n",
       " Interval(33.964, 35.505, THE MAILMAN BROUGHT A LETTER),\n",
       " Interval(35.798, 37.308, THE MOTHER HEARD THE BABY),\n",
       " Interval(37.479, 39.429, SHE FOUND HER PURSE IN THE TRASH),\n",
       " Interval(39.477, 41.456, THE TABLE HAS THREE LEGS),\n",
       " Interval(41.565, 43.304, THE CHILDREN WAVED AT THE TRAIN),\n",
       " Interval(45.1, 47.02, THE GIRL IS FIXING HER DRESS),\n",
       " Interval(47.081, 48.5, IT'S TIME TO GO TO BED),\n",
       " Interval(48.711, 50.62, MOTHER READ THE INSTRUCTIONS),\n",
       " Interval(50.762, 52.392, THE DOG IS EATING SOME MEAT),\n",
       " Interval(52.751, 54.251, FATHER FORGOT THE BREAD),\n",
       " Interval(54.487, 55.926, THE ROAD GOES UP A HILL),\n",
       " Interval(56.081, 57.692, THE PAINTER USES A BRUSH),\n",
       " Interval(57.865, 59.405, THE FAMILY BOUGHT A HOUSE),\n",
       " Interval(68.926, 70.835, THEY HAD TWO EMPTY BOTTLES),\n",
       " Interval(74.492, 76.442, THE HOUSE HAD NINE BEDROOMS)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example=r\"..\\data\\raw_L1\\ALL_ENG_ENG_HT1\\ALL_133_M_ENG_ENG_HT1.wav\"\n",
    "tg = textgrid.TextGrid.fromFile(example[:-3]+\"TextGrid\")\n",
    "tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "\n",
    "[tg_sentence[each] for each in set12_keywords_list.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_035_M_CMN_ENG_HT1.wav'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_CMN_ENG_HT1_pathset_exposure_specific[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1\n",
      "1 2 2\n"
     ]
    }
   ],
   "source": [
    "for _, (key,value) in enumerate({1:1,2:2}.items()):\n",
    "    print(_,key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [2], 2: [2, 3, 4]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist={1:[1,2],2:[2,3,4]}\n",
    "del alist[1][0]\n",
    "alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HE HUNG UP HIS RAINCOAT': [Interval(33.096, 33.335, HUNG),\n",
       "  Interval(34.475, 35.105, RAINCOAT)],\n",
       " 'THE MAILMAN BROUGHT A LETTER': [Interval(35.428, 35.967, MAILMAN),\n",
       "  Interval(35.967, 36.288, BROUGHT),\n",
       "  Interval(36.358, 36.867, LETTER)],\n",
       " 'THE MOTHER HEARD THE BABY': [Interval(37.178, 37.677, MOTHER),\n",
       "  Interval(37.857, 38.177, HEARD),\n",
       "  Interval(38.287, 38.707, BABY)],\n",
       " 'SHE FOUND HER PURSE IN THE TRASH': [Interval(39.157, 39.467, FOUND),\n",
       "  Interval(39.546, 40.116, PURSE),\n",
       "  Interval(40.727, 41.357, TRASH)],\n",
       " 'THE TABLE HAS THREE LEGS': [Interval(41.655, 42.104, TABLE),\n",
       "  Interval(42.104, 42.324, HAS),\n",
       "  Interval(42.364, 42.714, THREE),\n",
       "  Interval(42.714, 43.214, LEGS)],\n",
       " 'THE CHILDREN WAVED AT THE TRAIN': [Interval(43.559, 43.888, CHILDREN),\n",
       "  Interval(43.888, 44.229, WAVED),\n",
       "  Interval(44.668, 44.989, TRAIN)],\n",
       " 'THE GIRL IS FIXING HER DRESS': [Interval(47.605, 48.014, GIRL),\n",
       "  Interval(48.184, 48.594, FIXING),\n",
       "  Interval(48.755, 49.314, DRESS)],\n",
       " \"IT'S TIME TO GO TO BED\": [Interval(49.686, 49.916, TIME),\n",
       "  Interval(50.085, 50.245, GO),\n",
       "  Interval(50.345, 50.615, BED)],\n",
       " 'MOTHER READ THE INSTRUCTIONS': [Interval(50.883, 51.412, MOTHER),\n",
       "  Interval(51.522, 51.833, READ),\n",
       "  Interval(52.022, 52.883, INSTRUCTIONS)],\n",
       " 'THE DOG IS EATING SOME MEAT': [Interval(53.245, 53.594, DOG),\n",
       "  Interval(53.984, 54.224, EATING),\n",
       "  Interval(54.224, 54.445, SOME),\n",
       "  Interval(54.445, 54.665, MEAT)],\n",
       " 'FATHER FORGOT THE BREAD': [Interval(55.079, 55.43, FATHER),\n",
       "  Interval(55.43, 55.869, FORGOT),\n",
       "  Interval(56.039, 56.289, BREAD)],\n",
       " 'THE ROAD GOES UP A HILL': [Interval(56.799, 57.178, ROAD),\n",
       "  Interval(57.178, 57.498, GOES),\n",
       "  Interval(57.788, 58.159, HILL)],\n",
       " 'THE PAINTER USES A BRUSH': [Interval(58.363, 58.753, PAINTER),\n",
       "  Interval(58.753, 59.152, USES),\n",
       "  Interval(59.202, 59.712, BRUSH)],\n",
       " 'THE FAMILY BOUGHT A HOUSE': [Interval(59.956, 60.465, FAMILY),\n",
       "  Interval(60.715, 60.935, BOUGHT),\n",
       "  Interval(60.995, 61.606, HOUSE)],\n",
       " 'THEY HAD TWO EMPTY BOTTLES': [Interval(70.167, 70.376, HAD),\n",
       "  Interval(70.376, 70.566, TWO),\n",
       "  Interval(70.566, 70.936, EMPTY),\n",
       "  Interval(70.936, 71.646, BOTTLES)],\n",
       " 'THE HOUSE HAD NINE BEDROOMS': [Interval(75.761, 76.191, HOUSE),\n",
       "  Interval(76.191, 76.4, HAD),\n",
       "  Interval(76.4, 76.751, NINE),\n",
       "  Interval(76.88, 77.691, BEDROOMS)]}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_test_set_dict(path, keywords_list, model, processor):\n",
    "    tg = textgrid.TextGrid.fromFile(path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "\n",
    "    tg_sentences=[tg_sentence[int(each)] for each in keywords_list.keys()]\n",
    "    \n",
    "    tg_word_dict={}\n",
    "    for each_word in tg_word:\n",
    "        for each_sentence in tg_sentences:\n",
    "            if each_sentence.mark not in tg_word_dict.keys():\n",
    "                tg_word_dict[each_sentence.mark]=[]\n",
    "            if each_sentence.minTime <= each_word.minTime and each_sentence.maxTime >= each_word.maxTime:\n",
    "                tg_word_dict[each_sentence.mark].append(each_word)\n",
    "    \n",
    "    out_dict={}\n",
    "    for _,(each_sentence, words_tg) in enumerate(tg_word_dict.items()):\n",
    "        if each_sentence not in out_dict.keys():\n",
    "            out_dict[each_sentence]=[]\n",
    "        for __,each_tg in enumerate(words_tg):\n",
    "            if each_tg.mark.lower() in list(keywords_list.items())[_][-1]:\n",
    "                out_dict[each_sentence].append(each_tg)\n",
    "    return out_dict\n",
    "\n",
    "tg_word_dict=get_test_set_dict(ALL_CMN_ENG_HT1_pathset_exposure_specific[0], set12_keywords_list, model, processor)\n",
    "tg_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787230"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave, sr = librosa.load(ALL_CMN_ENG_HT1_pathset_exposure_specific[0])\n",
    "wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "input=processor(wave_res, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "len(input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 0, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 17, 18, 21, 22, 23, 24, 25, 26, 27, 29, 32, 33, 34, 36, 37, 38, 39, 40, 42, 44, 49, 52, 53, 59, 60, 63, 65, 66, 100])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_phonemes = ['<pad>', '<s>', '</s>', '<unk>', 'p', 'b', 't', 'd', 'k', 'g', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'ʒ', \n",
    "                    'h', 'm', 'n', 'ŋ', 'l', 'ɹ', 'w', 'j', 'tʃ', 'dʒ', \n",
    "                    'i', 'ɪ', 'eɪ', 'ɛ', 'æ', 'ɑ', 'ʌ', 'ɔ', 'oʊ', 'ʊ', 'u', \n",
    "                    'ɜː', 'ə', 'aɪ', 'aʊ', 'ɔɪ']\n",
    "english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "english_phoneme_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 1,\n",
       " '<pad>': 0,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " 'n': 4,\n",
       " 's': 5,\n",
       " 't': 6,\n",
       " 'ə': 7,\n",
       " 'l': 8,\n",
       " 'a': 9,\n",
       " 'i': 10,\n",
       " 'k': 11,\n",
       " 'd': 12,\n",
       " 'm': 13,\n",
       " 'ɛ': 14,\n",
       " 'ɾ': 15,\n",
       " 'e': 16,\n",
       " 'ɪ': 17,\n",
       " 'p': 18,\n",
       " 'o': 19,\n",
       " 'ɐ': 20,\n",
       " 'z': 21,\n",
       " 'ð': 22,\n",
       " 'f': 23,\n",
       " 'j': 24,\n",
       " 'v': 25,\n",
       " 'b': 26,\n",
       " 'ɹ': 27,\n",
       " 'ʁ': 28,\n",
       " 'ʊ': 29,\n",
       " 'iː': 30,\n",
       " 'r': 31,\n",
       " 'w': 32,\n",
       " 'ʌ': 33,\n",
       " 'u': 34,\n",
       " 'ɡ': 35,\n",
       " 'æ': 36,\n",
       " 'aɪ': 37,\n",
       " 'ʃ': 38,\n",
       " 'h': 39,\n",
       " 'ɔ': 40,\n",
       " 'ɑː': 41,\n",
       " 'ŋ': 42,\n",
       " 'ɚ': 43,\n",
       " 'eɪ': 44,\n",
       " 'β': 45,\n",
       " 'uː': 46,\n",
       " 'y': 47,\n",
       " 'ɑ̃': 48,\n",
       " 'oʊ': 49,\n",
       " 'ᵻ': 50,\n",
       " 'eː': 51,\n",
       " 'θ': 52,\n",
       " 'aʊ': 53,\n",
       " 'ts': 54,\n",
       " 'oː': 55,\n",
       " 'ɔ̃': 56,\n",
       " 'ɣ': 57,\n",
       " 'ɜ': 58,\n",
       " 'ɑ': 59,\n",
       " 'dʒ': 60,\n",
       " 'əl': 61,\n",
       " 'x': 62,\n",
       " 'ɜː': 63,\n",
       " 'ç': 64,\n",
       " 'ʒ': 65,\n",
       " 'tʃ': 66,\n",
       " 'ɔː': 67,\n",
       " 'ɑːɹ': 68,\n",
       " 'ɛ̃': 69,\n",
       " 'ʎ': 70,\n",
       " 'ɔːɹ': 71,\n",
       " 'ʋ': 72,\n",
       " 'aː': 73,\n",
       " 'ɕ': 74,\n",
       " 'œ': 75,\n",
       " 'ø': 76,\n",
       " 'oːɹ': 77,\n",
       " 'ɲ': 78,\n",
       " 'yː': 79,\n",
       " 'ʔ': 80,\n",
       " 'iə': 81,\n",
       " 'i5': 82,\n",
       " 's.': 83,\n",
       " 'tɕ': 84,\n",
       " '??': 85,\n",
       " 'nʲ': 86,\n",
       " 'ɛː': 87,\n",
       " 'œ̃': 88,\n",
       " 'ɭ': 89,\n",
       " 'ɔø': 90,\n",
       " 'ʑ': 91,\n",
       " 'tʲ': 92,\n",
       " 'ɨ': 93,\n",
       " 'ɛɹ': 94,\n",
       " 'ts.': 95,\n",
       " 'rʲ': 96,\n",
       " 'ɪɹ': 97,\n",
       " 'ɭʲ': 98,\n",
       " 'i.5': 99,\n",
       " 'ɔɪ': 100,\n",
       " 'q': 101,\n",
       " 'sʲ': 102,\n",
       " 'u5': 103,\n",
       " 'ʊɹ': 104,\n",
       " 'iɜ': 105,\n",
       " 'a5': 106,\n",
       " 'iɛ5': 107,\n",
       " 'øː': 108,\n",
       " 'ʕ': 109,\n",
       " 'ja': 110,\n",
       " 'əɜ': 111,\n",
       " 'th': 112,\n",
       " 'ɑ5': 113,\n",
       " 'oɪ': 114,\n",
       " 'dʲ': 115,\n",
       " 'ə5': 116,\n",
       " 'tɕh': 117,\n",
       " 'ts.h': 118,\n",
       " 'mʲ': 119,\n",
       " 'ɯ': 120,\n",
       " 'dʑ': 121,\n",
       " 'vʲ': 122,\n",
       " 'e̞': 123,\n",
       " 'tʃʲ': 124,\n",
       " 'ei5': 125,\n",
       " 'o5': 126,\n",
       " 'onɡ5': 127,\n",
       " 'ɑu5': 128,\n",
       " 'iɑ5': 129,\n",
       " 'ai5': 130,\n",
       " 'aɪɚ': 131,\n",
       " 'kh': 132,\n",
       " 'ə1': 133,\n",
       " 'ʐ': 134,\n",
       " 'i2': 135,\n",
       " 'ʉ': 136,\n",
       " 'ħ': 137,\n",
       " 't[': 138,\n",
       " 'aɪə': 139,\n",
       " 'ʲ': 140,\n",
       " 'ju': 141,\n",
       " 'ə2': 142,\n",
       " 'u2': 143,\n",
       " 'oɜ': 144,\n",
       " 'pː': 145,\n",
       " 'iɛɜ': 146,\n",
       " 'ou5': 147,\n",
       " 'y5': 148,\n",
       " 'uɜ': 149,\n",
       " 'tː': 150,\n",
       " 'uo5': 151,\n",
       " 'd[': 152,\n",
       " 'uoɜ': 153,\n",
       " 'tsh': 154,\n",
       " 'ɑɜ': 155,\n",
       " 'ɵ': 156,\n",
       " 'i̪5': 157,\n",
       " 'uei5': 158,\n",
       " 'ɟ': 159,\n",
       " 'aɜ': 160,\n",
       " 'ɑɨ': 161,\n",
       " 'i.ɜ': 162,\n",
       " 'eʊ': 163,\n",
       " 'o2': 164,\n",
       " 'ɐ̃': 165,\n",
       " 'ä': 166,\n",
       " 'pʲ': 167,\n",
       " 'kʲ': 168,\n",
       " 'n̩': 169,\n",
       " 'ɒ': 170,\n",
       " 'ph': 171,\n",
       " 'ɑu2': 172,\n",
       " 'uɨ': 173,\n",
       " 'əɪ': 174,\n",
       " 'ɫ': 175,\n",
       " 'ɬ': 176,\n",
       " 'yɜ': 177,\n",
       " 'bʲ': 178,\n",
       " 'ɑ2': 179,\n",
       " 's̪': 180,\n",
       " 'aiɜ': 181,\n",
       " 'χ': 182,\n",
       " 'ɐ̃ʊ̃': 183,\n",
       " '1': 184,\n",
       " 'ə4': 185,\n",
       " 'yæɜ': 186,\n",
       " 'a2': 187,\n",
       " 'ɨː': 188,\n",
       " 't̪': 189,\n",
       " 'iouɜ': 190,\n",
       " 'ũ': 191,\n",
       " 'onɡɜ': 192,\n",
       " 'aɨ': 193,\n",
       " 'iɛ2': 194,\n",
       " 'ɔɨ': 195,\n",
       " 'ɑuɜ': 196,\n",
       " 'o̞': 197,\n",
       " 'ei2': 198,\n",
       " 'iou2': 199,\n",
       " 'c': 200,\n",
       " 'kː': 201,\n",
       " 'y2': 202,\n",
       " 'ɖ': 203,\n",
       " 'oe': 204,\n",
       " 'dˤ': 205,\n",
       " 'yɛɜ': 206,\n",
       " 'əʊ': 207,\n",
       " 'S': 208,\n",
       " 'ɡʲ': 209,\n",
       " 'onɡ2': 210,\n",
       " 'u\"': 211,\n",
       " 'eiɜ': 212,\n",
       " 'ʈ': 213,\n",
       " 'ɯᵝ': 214,\n",
       " 'iou5': 215,\n",
       " 'dZ': 216,\n",
       " 'r̝̊': 217,\n",
       " 'i.2': 218,\n",
       " 'tS': 219,\n",
       " 's^': 220,\n",
       " 'ʝ': 221,\n",
       " 'yə5': 222,\n",
       " 'iɑɜ': 223,\n",
       " 'uə5': 224,\n",
       " 'pf': 225,\n",
       " 'ɨu': 226,\n",
       " 'iɑ2': 227,\n",
       " 'ou2': 228,\n",
       " 'ər2': 229,\n",
       " 'fʲ': 230,\n",
       " 'ai2': 231,\n",
       " 'r̝': 232,\n",
       " 'uəɜ': 233,\n",
       " 'ɳ': 234,\n",
       " 'əɨ': 235,\n",
       " 'ua5': 236,\n",
       " 'uɪ': 237,\n",
       " 'ɽ': 238,\n",
       " 'bː': 239,\n",
       " 'yu5': 240,\n",
       " 'uo2': 241,\n",
       " 'yɛ5': 242,\n",
       " 'l̩': 243,\n",
       " 'ɻ': 244,\n",
       " 'ərɜ': 245,\n",
       " 'ʂ': 246,\n",
       " 'i̪2': 247,\n",
       " 'ouɜ': 248,\n",
       " 'uaɜ': 249,\n",
       " 'a.': 250,\n",
       " 'a.ː': 251,\n",
       " 'yæ5': 252,\n",
       " 'dː': 253,\n",
       " 'r̩': 254,\n",
       " 'ee': 255,\n",
       " 'ɪu': 256,\n",
       " 'ər5': 257,\n",
       " 'i̪ɜ': 258,\n",
       " 'æi': 259,\n",
       " 'u:': 260,\n",
       " 'i.ː': 261,\n",
       " 't^': 262,\n",
       " 'o1': 263,\n",
       " 'ɪ^': 264,\n",
       " 'ai': 265,\n",
       " 'ueiɜ': 266,\n",
       " 'æː': 267,\n",
       " 'ɛɪ': 268,\n",
       " 'eə': 269,\n",
       " 'i.': 270,\n",
       " 'ɴ': 271,\n",
       " 'ie': 272,\n",
       " 'ua2': 273,\n",
       " 'ɑ1': 274,\n",
       " 'o4': 275,\n",
       " 'tʃː': 276,\n",
       " 'o:': 277,\n",
       " 'ɑ:': 278,\n",
       " 'u1': 279,\n",
       " 'N': 280,\n",
       " 'i̪1': 281,\n",
       " 'au': 282,\n",
       " 'yæ2': 283,\n",
       " 'u.': 284,\n",
       " 'qː': 285,\n",
       " 'yəɜ': 286,\n",
       " 'y:': 287,\n",
       " 'kʰ': 288,\n",
       " 'tʃʰ': 289,\n",
       " 'iʊ': 290,\n",
       " 'sx': 291,\n",
       " 'õ': 292,\n",
       " 'uo': 293,\n",
       " 'tʰ': 294,\n",
       " 'uai5': 295,\n",
       " 'bʰ': 296,\n",
       " 'u.ː': 297,\n",
       " 'uə2': 298,\n",
       " 'ʊə': 299,\n",
       " 'd^': 300,\n",
       " 's̪ː': 301,\n",
       " 'yiɜ': 302,\n",
       " 'dʰ': 303,\n",
       " 'r.': 304,\n",
       " 'oe:': 305,\n",
       " 'i1': 306,\n",
       " 'ɟː': 307,\n",
       " 'yu2': 308,\n",
       " 'nʲʲ': 309,\n",
       " 'i̪4': 310,\n",
       " 'uei2': 311,\n",
       " 'tsʲ': 312,\n",
       " 'ɸ': 313,\n",
       " 'ĩ': 314,\n",
       " 'ɑ4': 315,\n",
       " 't̪ː': 316,\n",
       " 'eɑ': 317,\n",
       " 'u4': 318,\n",
       " 'e:': 319,\n",
       " 'tsː': 320,\n",
       " 'ʈʰ': 321,\n",
       " 'ɡʰ': 322,\n",
       " 'ɯɯ': 323,\n",
       " 'dʒʲ': 324,\n",
       " 'ʂʲ': 325,\n",
       " 'X': 326,\n",
       " 'ɵː': 327,\n",
       " 'uaiɜ': 328,\n",
       " 'tɕʲ': 329,\n",
       " 'ã': 330,\n",
       " 't^ː': 331,\n",
       " 'ẽː': 332,\n",
       " 'yɛ2': 333,\n",
       " 'cː': 334,\n",
       " 'i.1': 335,\n",
       " 'ɛʊ': 336,\n",
       " 'dˤdˤ': 337,\n",
       " 'dʒː': 338,\n",
       " 'i4': 339,\n",
       " 'ɡː': 340,\n",
       " 'yi': 341,\n",
       " 'ɕʲ': 342,\n",
       " 'ɟʰ': 343,\n",
       " 'pʰ': 344,\n",
       " 'dʑʲ': 345,\n",
       " 'yuɜ': 346,\n",
       " 'ua1': 347,\n",
       " 'ua4': 348,\n",
       " 'æiː': 349,\n",
       " 'ɐɐ': 350,\n",
       " 'ui': 351,\n",
       " 'iou1': 352,\n",
       " 'ʊː': 353,\n",
       " 'a1': 354,\n",
       " 'iou4': 355,\n",
       " 'cʰ': 356,\n",
       " 'iɛ1': 357,\n",
       " 'yə2': 358,\n",
       " 'ɖʰ': 359,\n",
       " 'ẽ': 360,\n",
       " 'ʒʲ': 361,\n",
       " 'ää': 362,\n",
       " 'ər4': 363,\n",
       " 'iːː': 364,\n",
       " 'ɪː': 365,\n",
       " 'iɑ1': 366,\n",
       " 'ər1': 367,\n",
       " 'œː': 368,\n",
       " 'øi': 369,\n",
       " 'ɪuː': 370,\n",
       " 'cʰcʰ': 371,\n",
       " 'əː1': 372,\n",
       " 'iː1': 373,\n",
       " 'ũ': 374,\n",
       " 'kʰː': 375,\n",
       " 'o̞o̞': 376,\n",
       " 'xʲ': 377,\n",
       " 'ou1': 378,\n",
       " 'iɛ4': 379,\n",
       " 'e̞e̞': 380,\n",
       " 'y1': 381,\n",
       " 'dzː': 382,\n",
       " 'dʲʲ': 383,\n",
       " 'dʰː': 384,\n",
       " 'ɯᵝɯᵝ': 385,\n",
       " 'lː': 386,\n",
       " 'uo1': 387,\n",
       " 'i.4': 388,\n",
       " 'i:': 389,\n",
       " 'yɛ5ʲ': 390,\n",
       " 'a4': 391}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor_P.tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtg_word_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "tg_word_dict.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.ones(out_encoder.shape[-1], dtype=bool)\n",
    "mask[list(english_phoneme_dict.values())] = False\n",
    "out_encoder[:, :, mask]=0\n",
    "outind=torch.argmax(out_encoder,dim=-1).cpu().numpy()\n",
    "phonemeindex = CTC_index(processor,outind)\n",
    "phonemeindex\n",
    "#transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "#transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 'iɑ5']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut=input[:,round(list(tg_word_dict.values())[0][0].minTime*16000):round(list(tg_word_dict.values())[0][0].maxTime*16000)]\n",
    "with torch.no_grad():\n",
    "    out_encoder=model_P(cut.to(device)).logits\n",
    "outind=torch.argmax(out_encoder,dim=-1).cpu().numpy()\n",
    "transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_word_dict\n",
    "def get_test_set_diphone(path, keywords_list, model, processor):\n",
    "    wave, sr = librosa.load(path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    input=processor(wave_res, sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.49it/s]\n",
      "100%|██████████| 5/5 [00:01<00:00,  3.73it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_test_set_diphone(path, keywords_list, model, processor):\n",
    "    tg = textgrid.TextGrid.fromFile(path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "\n",
    "    tg_sentences=[tg_sentence[each] for each in keywords_list.keys()]\n",
    "    \n",
    "    tg_word_dict={}\n",
    "    for each_word in tg_word:\n",
    "        for each_sentence in tg_sentences:\n",
    "            if each_sentence.mark not in tg_word_dict.keys():\n",
    "                tg_word_dict[each_sentence.mark]=[]\n",
    "            if each_sentence.minTime <= each_word.minTime and each_sentence.maxTime >= each_word.maxTime:\n",
    "                tg_word_dict[each_sentence.mark].append(each_word)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "    wave, sr = librosa.load(path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "\n",
    "    input=processor(wave_res, sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().numpy()\n",
    "        out_encoder=model(input).logits\n",
    "    #outind=torch.argmax(out_encoder,dim=-1).cpu().numpy()\n",
    "    \n",
    "    sentence_dict={}\n",
    "    for each_sentence in tg_sentence:\n",
    "        if each_sentence.mark not in sentence_dict.keys():\n",
    "            sentence_dict[each_sentence.mark] = []\n",
    "        \n",
    "        sentence_start=round(each_sentence.minTime/sentence16_end_time*out_FE.shape[0])+1\n",
    "        sentence_end=round(each_sentence.maxTime/sentence16_end_time*out_FE.shape[0])+1\n",
    "        \n",
    "        outind=torch.argmax(out_encoder[:,sentence_start:sentence_end,:],dim=-1).cpu().numpy()\n",
    "        transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        each_FE = out_FE[sentence_start:sentence_end,:]\n",
    "        for i in range(len(transcription)-1):\n",
    "            key = transcription[i] + transcription[i + 1]\n",
    "            sentence_dict[each_sentence.mark].append((key,np.vstack((each_FE[phonemeindex[i]], each_FE[phonemeindex[i + 1]]))))\n",
    "\n",
    "    return sentence_dict\n",
    "\n",
    "ALL_ENG_ENG_HT1_path=r\"..\\data\\raw\\ALL_ENG_ENG_HT1\"\n",
    "ALL_ENG_ENG_HT1_pathset=get_pathset(ALL_ENG_ENG_HT1_path)\n",
    "ALL_ENG_ENG_HT2_path=r\"..\\data\\raw\\ALL_ENG_ENG_HT2\"\n",
    "ALL_ENG_ENG_HT2_pathset=get_pathset(ALL_ENG_ENG_HT2_path)\n",
    "\n",
    "ALL_CMN_ENG_HT1_path=r\"..\\data\\raw\\ALL_CMN_ENG_HT1\"\n",
    "ALL_CMN_ENG_HT1_pathset=get_pathset(ALL_CMN_ENG_HT1_path)\n",
    "ALL_CMN_ENG_HT2_path=r\"..\\data\\raw\\ALL_CMN_ENG_HT2\"\n",
    "ALL_CMN_ENG_HT2_pathset=get_pathset(ALL_CMN_ENG_HT2_path)\n",
    "\n",
    "\n",
    "ALL_ENG_ENG_HT1_pathset_exposure_control=['..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_055_M_ENG_ENG_HT1.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_066_M_ENG_ENG_HT1.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_070_M_ENG_ENG_HT1.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_131_M_ENG_ENG_HT1.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT1\\\\ALL_133_M_ENG_ENG_HT1.wav']\n",
    "ALL_ENG_ENG_HT2_pathset_exposure_control=['..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT2\\\\ALL_055_M_ENG_ENG_HT2.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT2\\\\ALL_066_M_ENG_ENG_HT2.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT2\\\\ALL_070_M_ENG_ENG_HT2.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT2\\\\ALL_131_M_ENG_ENG_HT2.wav','..\\\\data\\\\raw\\\\ALL_ENG_ENG_HT2\\\\ALL_133_M_ENG_ENG_HT2.wav']\n",
    "\n",
    "ALL_CMN_ENG_HT1_pathset_exposure_multi=ALL_CMN_ENG_HT1_pathset[-5:]\n",
    "ALL_CMN_ENG_HT2_pathset_exposure_multi=ALL_CMN_ENG_HT2_pathset[-5:]\n",
    "\n",
    "ALL_CMN_ENG_HT1_pathset_exposure_specific=ALL_CMN_ENG_HT1_pathset[-4:]\n",
    "ALL_CMN_ENG_HT2_pathset_exposure_specific=ALL_CMN_ENG_HT2_pathset[-4:]\n",
    "\n",
    "ALL_CMN_ENG_HT1_pathset_exposure_single=ALL_CMN_ENG_HT1_pathset[-5:]\n",
    "ALL_CMN_ENG_HT2_pathset_exposure_single=ALL_CMN_ENG_HT2_pathset[-5:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ALL_ENG_ENG_HT1_pathset_exposure_control_diphone=get_set_diphone(ALL_ENG_ENG_HT1_pathset_exposure_control,model_P,processor_P)\n",
    "ALL_ENG_ENG_HT2_pathset_exposure_control_diphone=get_set_diphone(ALL_ENG_ENG_HT2_pathset_exposure_control,model_P,processor_P)\n",
    "\n",
    "ALL_035_M_CMN_ENG_HT1_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT1_pathset_exposure_specific[0],model_P,processor_P)\n",
    "ALL_035_M_CMN_ENG_HT2_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT2_pathset_exposure_specific[0],model_P,processor_P)\n",
    "\n",
    "ALL_037_M_CMN_ENG_HT1_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT1_pathset_exposure_specific[1],model_P,processor_P)\n",
    "ALL_037_M_CMN_ENG_HT2_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT2_pathset_exposure_specific[1],model_P,processor_P)\n",
    "\n",
    "ALL_039_M_CMN_ENG_HT1_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT1_pathset_exposure_specific[2],model_P,processor_P)\n",
    "ALL_039_M_CMN_ENG_HT2_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT2_pathset_exposure_specific[2],model_P,processor_P)\n",
    "\n",
    "ALL_043_M_CMN_ENG_HT1_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT1_pathset_exposure_specific[3],model_P,processor_P)\n",
    "ALL_043_M_CMN_ENG_HT2_test_specific_diphone=get_test_set_diphone(ALL_CMN_ENG_HT2_pathset_exposure_specific[3],model_P,processor_P)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
