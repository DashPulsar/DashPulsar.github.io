{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet_to_ipa = {\n",
    "    'W': 'w',\n",
    "    'UW0': 'u',\n",
    "    'N': 'n',\n",
    "    'AW1': 'aʊ',\n",
    "    'OW0': 'oʊ',\n",
    "    'IY0': 'i',\n",
    "    'L': 'l',\n",
    "    'IY1': 'i',\n",
    "    'SH': 'ʃ',\n",
    "    'AE2': 'æ',\n",
    "    'AO1': 'ɒ',\n",
    "    'HH': 'h',\n",
    "    'V': 'v',\n",
    "    'AA2': 'ɑ',\n",
    "    'EY2': 'eɪ',\n",
    "    'AE1': 'æ',\n",
    "    'Z': 'z',\n",
    "    'UW2': 'u',\n",
    "    'D': 'd',\n",
    "    'AH2': 'ʌ',\n",
    "    'M': 'm',\n",
    "    'B': 'b',\n",
    "    'IY2': 'i',\n",
    "    'OY1': 'ɔɪ',\n",
    "    'F': 'f',\n",
    "    'CH': 'tʃ',\n",
    "    'Y': 'j',\n",
    "    'TH': 'θ',\n",
    "    'ER1': 'ɜ',\n",
    "    'ER0': 'ɜ',\n",
    "    'AO2': 'ɒ',\n",
    "    'JH': 'dʒ',\n",
    "    'UW1': 'u',\n",
    "    'P': 'p',\n",
    "    'AY1': 'aɪ',\n",
    "    'IH2': 'ɪ',\n",
    "    'T': 't',\n",
    "    'K': 'k',\n",
    "    'AO0': 'ɒ',\n",
    "    'DH': 'ð',\n",
    "    'OW2': 'oʊ',\n",
    "    'EH1': 'e',\n",
    "    'G': 'ɡ',\n",
    "    'IH0': 'ɪ',\n",
    "    'AH1': 'ʌ',\n",
    "    'EY1': 'eɪ',\n",
    "    'AH0': 'ʌ',\n",
    "    'NG': 'ŋ',\n",
    "    'AA1': 'ɑ',\n",
    "    'IH1': 'ɪ',\n",
    "    'S': 's',\n",
    "    'OW1': 'oʊ',\n",
    "    'UH1': 'ʊ',\n",
    "    'R': 'ɹ'\n",
    "}\n",
    "\n",
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "\n",
    "def CTC_index(processor,outind):\n",
    "    meaningful_ids = []\n",
    "    meaningful_indices = []\n",
    "    previous_id = -1  \n",
    "    blank_token_id = processor.tokenizer.pad_token_id  \n",
    "    for i, token_id in enumerate(outind[0]):  \n",
    "        if token_id != previous_id and token_id != blank_token_id:\n",
    "            meaningful_ids.append(token_id.item())  \n",
    "            meaningful_indices.append(i)  \n",
    "        previous_id = token_id\n",
    "    \n",
    "    return meaningful_indices\n",
    "\n",
    "def get_training_paths(TrainingTalkerID,all_path):\n",
    "    path_list=[]\n",
    "    TalkerID=[]\n",
    "    for each_ID in TrainingTalkerID.split(\", \"):\n",
    "        if each_ID[:3]==\"CMN\":\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_CMN_ENG_HT1\")\n",
    "        else:\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_ENG_ENG_HT1\")\n",
    "    \n",
    "    for each_path in TalkerID:\n",
    "        for i in all_path:\n",
    "            if each_path in i:\n",
    "                path_list.append(i)\n",
    "                break\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)       \n",
    "    aligned_seq1, aligned_seq2 = [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')  \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1]\n",
    "\n",
    "def align_sequences_with_index(seq1, seq2, index_list):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)\n",
    "\n",
    "\n",
    "    aligned_seq1, aligned_seq2, aligned_index_list = [], [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')\n",
    "            aligned_index_list.append(index_list[j - 1] + 1) \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(index_list[j - 1]) \n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(index_list[j - 1])  \n",
    "            i -= 1\n",
    "            j -= 1\n",
    "\n",
    "\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1], aligned_index_list[::-1]\n",
    "\n",
    "def build_exposure_set(paths, native_dict, set_list, model,processor):\n",
    "    diphone_dict={}\n",
    "    uniphone_dict={}\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    english_phoneme_dict.values()\n",
    "    for each_sentence in paths:\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "        tg_sentence = [each for _,each in enumerate(tg_sentence) if _ in set_list]\n",
    "        '''sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]'''\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            #each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_tg.minTime<=i.minTime and each_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "            start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            '''aligned_seq1, aligned_seq2 ,phonemeindex= align_sequences_with_index(each_phonemes,transcription,phonemeindex)\n",
    "            if phonemeindex[0]>phonemeindex[1]:\n",
    "                phonemeindex[0]=0\n",
    "            elif phonemeindex[0]==phonemeindex[1] and phonemeindex[1]>phonemeindex[2]:\n",
    "                phonemeindex[0],phonemeindex[1]=0,1\n",
    "            #aligned_seq1, aligned_seq2 = align_sequences(each_phonemes,transcription)\n",
    "            if not len(aligned_seq1)==len(aligned_seq2)==len(phonemeindex):\n",
    "                print(len(aligned_seq1),len(aligned_seq2),len(phonemeindex))\n",
    "                print(aligned_seq1,\"\\n\",aligned_seq2,\"\\n\",phonemeindex)\n",
    "                raise IndexError(\"length unmatch\")'''\n",
    "            \n",
    "            for i in range(len(transcription)-1):#aligned_seq1transcription\n",
    "                key = (transcription[i], transcription[i + 1])\n",
    "                #key = transcription[i] + transcription[i + 1]\n",
    "                if key not in diphone_dict:\n",
    "                    diphone_dict[key] = []\n",
    "                #try:\n",
    "                diphone_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    " \n",
    "                #except:\n",
    "                    #print(each_tg)\n",
    "                    #print(aligned_seq1)\n",
    "                    #print(aligned_seq2)\n",
    "                    #print(transcription)\n",
    "                    #print(phonemeindex)\n",
    "                    #print(key)\n",
    "                    #out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i]])))\n",
    "            for i in range(len(transcription)):\n",
    "                if transcription[i] not in uniphone_dict:\n",
    "                    uniphone_dict[transcription[i]]=[]\n",
    "                uniphone_dict[transcription[i]].append(out_FE[phonemeindex[i]])\n",
    "            '''for i in range(len(transcription)-1):\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in native_dict:\n",
    "                    native_dict[key] = []\n",
    "                native_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))'''\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return diphone_dict,uniphone_dict#native_dict\n",
    "    #'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_032_M_CMN_ENG_HT1.wav'\n",
    "    \n",
    "    \n",
    "def get_test_list(file_path,key_word,sentenceID,model,processor):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    sentenceID=int(sentenceID[-3:])-1\n",
    "    #file_path= f'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\{file_path[:-5]}.wav'\n",
    "    \n",
    "    tg = textgrid.TextGrid.fromFile(file_path[:-3]+\"TextGrid\")\n",
    "    tg_sentence = [i for i in tg[0] if i.mark!=\"\"][sentenceID]\n",
    "    \n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    \n",
    "    wave, sr = librosa.load(file_path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    \n",
    "\n",
    "    for each_word_tg in tg_word:\n",
    "        if each_word_tg.minTime >= tg_sentence.minTime and each_word_tg.maxTime <= tg_sentence.maxTime:\n",
    "            #print(each_word_tg.mark.lower(),key_word)\n",
    "            if each_word_tg.mark.lower()==key_word:\n",
    "                start=each_word_tg.minTime\n",
    "                end=each_word_tg.maxTime\n",
    "                break\n",
    "                #print(\"start:\",start,\"end:\",end)\n",
    "    #word_length=len(wave_res)/16000\n",
    "    out_list=[]\n",
    "    each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_word_tg.minTime<=i.minTime and each_word_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "    sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "    word_cut_start=start-tg_sentence.minTime\n",
    "    word_cut_end=end-tg_sentence.minTime\n",
    "    \n",
    "    input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_encoder=model(input.to(device)).logits\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().numpy()\n",
    "    \n",
    "    word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "    word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "    \n",
    "    selected=out_encoder[:,word_start:word_end,:]\n",
    "    mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "    mask[list(english_phoneme_dict.values())] = False\n",
    "    selected[:, :, mask] = 0\n",
    "    outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "    phonemeindex = CTC_index(processor,outind)\n",
    "    transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "    #aligned_seq1, aligned_seq2 ,phonemeindex_= align_sequences_with_index(each_phonemes,transcription,phonemeindex)\n",
    "    \n",
    "            \n",
    "    if len(phonemeindex)<2:\n",
    "        each_FE = out_FE[word_start:,:]\n",
    "        '''each_FE = out_FE[word_start:,:]\n",
    "        selected=out_encoder[:,word_start:,:]\n",
    "        mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "        mask[list(english_phoneme_dict.values())] = False\n",
    "        selected[:, :, mask] = 0\n",
    "        outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        transcription = processor_P.batch_decode(outind)[0].split(\" \")'''\n",
    "        \n",
    "        diphone_key = (transcription[0])\n",
    "        out_list.append((diphone_key, each_FE[phonemeindex[0]]))#np.vstack((each_FE[phonemeindex[0]], each_FE[phonemeindex[0]]))))\n",
    "\n",
    "    else:\n",
    "        each_FE = out_FE[word_start:word_end,:]\n",
    "        for i in range(len(transcription)-1):\n",
    "            diphone_key = (transcription[i] , transcription[i + 1])\n",
    "        out_list.append((diphone_key, np.vstack((each_FE[phonemeindex[i]], each_FE[phonemeindex[i + 1]]))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_measure(df, model, processor):\n",
    "    sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count =[], [], [], [], []\n",
    "    confusion_matrix_list, wav2vec_acc_list=[],[]\n",
    "    X_list=[]\n",
    "    Y_list=[]\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    \n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    \n",
    "    phoneme_vocab = {phoneme: idx for idx, phoneme in enumerate(english_phonemes)}\n",
    "    train_set_dict={}\n",
    "    test_word_dict={}\n",
    "    test_matrix_dict={}\n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        filename_loc=df.columns.get_loc(\"Filename\")\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "        \n",
    "        all_path=get_pathset(r\"..\\data\\raw\")\n",
    "        #all_ENG_ENG_pathset=[s.replace(\"raw_L1\", \"raw\") for s in get_pathset(r\"..\\data\\raw_L1\")]\n",
    "        \n",
    "        set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "        set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "            train_set=set2_list\n",
    "            test_set=set1_list\n",
    "        else:\n",
    "            train_set=set1_list\n",
    "            test_set=set2_list\n",
    "        \n",
    "        #print(each_[filename_loc])\n",
    "        test_file = [each for each in all_path if os.path.split(each_[filename_loc])[-1][:-5] in each]\n",
    "        #print(test_file)\n",
    "        key_word = each_[keyword_loc] #string\n",
    "        TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "        \n",
    "        #if training_files_path[0] in all_ENG_ENG_pathset:\n",
    "        #    training_dict=copy.deepcopy(all_eng_dict)\n",
    "        #else:\n",
    "        \n",
    "        if TrainingTalkerID not in train_set_dict:\n",
    "            train_set_dict[TrainingTalkerID]={}\n",
    "            \n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] not in train_set_dict[TrainingTalkerID]:\n",
    "            training_dict_di, training_dict_uni=build_exposure_set(training_files_path, {}, train_set, model, processor)#copy.deepcopy(all_eng_dict)\n",
    "            train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]=(copy.deepcopy(training_dict_di),copy.deepcopy(training_dict_uni))\n",
    "        else:\n",
    "            training_dict_di, training_dict_uni=train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if test_file[0] not in test_word_dict:\n",
    "            test_word_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_word_dict[test_file[0]]:\n",
    "            test_word_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_word_dict[test_file[0]][sentenceID]:\n",
    "            test_list = get_test_list(test_file[0], key_word, sentenceID, model, processor)\n",
    "            test_word_dict[test_file[0]][sentenceID][key_word]=copy.deepcopy(test_list)\n",
    "        else:\n",
    "            test_list=test_word_dict[test_file[0]][sentenceID][key_word]\n",
    "        \n",
    "        if test_file[0] not in test_matrix_dict:\n",
    "            test_matrix_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_matrix_dict[test_file[0]]:\n",
    "            test_matrix_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_matrix_dict[test_file[0]][sentenceID]:\n",
    "            \n",
    "            tg = textgrid.TextGrid.fromFile(test_file[0][:-3]+\"TextGrid\")\n",
    "            tg_sentence = [i for i in tg[0] if i.mark!=\"\"][int(sentenceID[-3:])-1]\n",
    "            tg_word = [i for i in tg[1] if tg_sentence.minTime<=i.minTime and tg_sentence.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark.lower()==key_word][0]\n",
    "            \n",
    "            each_word_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if tg_word.minTime<=i.minTime and tg_word.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "            \n",
    "            sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "            word_cut_start=tg_word.minTime-tg_sentence.minTime\n",
    "            word_cut_end=tg_word.maxTime-tg_sentence.minTime\n",
    "            wave, sr = librosa.load(test_file[0])\n",
    "            wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "            input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder=model(input.to(device)).logits\n",
    "                \n",
    "            word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "            word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "            selected=out_encoder[:,word_start:word_end,:]\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #phonemeindex = CTC_index(processor,outind)\n",
    "            transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "            X_=each_word_phonemes\n",
    "            Y_=transcription\n",
    "            #print(each_word_phonemes,\"\\n\",transcription)\n",
    "            aligned_seq1, aligned_seq2 = align_sequences(each_word_phonemes,transcription)\n",
    "            \n",
    "            N=len(list(phoneme_vocab.keys()))\n",
    "            confusion_matrix = np.zeros((N, N), dtype=int)\n",
    "            #print(confusion_matrix.shape)\n",
    "            for true_phoneme, predicted_phoneme in zip(aligned_seq1, aligned_seq2):\n",
    "                if predicted_phoneme in english_phonemes[:4]:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[true_phoneme]\n",
    "                else:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[predicted_phoneme]\n",
    "                #print(true_idx, predicted_idx)\n",
    "                confusion_matrix[predicted_idx,true_idx] += 1\n",
    "            \n",
    "            \n",
    "            phoneme_error = [1 if aligned_seq1[_]==aligned_seq2[_] else 0 for _ in range(len(aligned_seq1))]\n",
    "            #print(np.array(list(english_phoneme_dict.values())))\n",
    "            #confusion_matrix = #out_encoder[:,word_start:word_end,list(english_phoneme_dict.values())].cpu().numpy()\n",
    "            test_matrix_dict[test_file[0]][sentenceID][key_word]=(X_,Y_,phoneme_error, confusion_matrix)\n",
    "        else:\n",
    "            X_=test_matrix_dict[test_file[0]][sentenceID][key_word][0]\n",
    "            Y_=test_matrix_dict[test_file[0]][sentenceID][key_word][1]\n",
    "            phoneme_error = test_matrix_dict[test_file[0]][sentenceID][key_word][2]\n",
    "            confusion_matrix = copy.deepcopy(test_matrix_dict[test_file[0]][sentenceID][key_word][3])\n",
    "        \n",
    "        # word level, list\n",
    "        sim_max=[]\n",
    "        sim_std=[]\n",
    "        sim_mean=[]\n",
    "        isincluded=[]\n",
    "        #sim_count=[]\n",
    "        for _, each_diphone in enumerate(test_list):\n",
    "            sims=[]\n",
    "            if len(each_diphone[0])==1:\n",
    "                if each_diphone[0][0] in training_dict_uni.keys():\n",
    "                    for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                        current_test_uni=each_diphone[1].ravel()\n",
    "                        current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                        current_e_uni=each_vec.ravel()\n",
    "                        current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                        sim=np.exp(-0.1*euclidean(current_test_uni,current_e_uni))\n",
    "                        sims.append(sim)\n",
    "                        isincluded.append(0)\n",
    "                        \n",
    "                else:\n",
    "                    raise IndexError(\"Uniphone unmatch\")\n",
    "                sim_max.append(np.max(sims))\n",
    "                sim_std.append(np.std(sims))\n",
    "                sim_mean.append(np.mean(sims))\n",
    "            else:\n",
    "                if each_diphone[0] in training_dict_di.keys():\n",
    "                    for each_vec in training_dict_di[each_diphone[0]]:\n",
    "                        sim=np.exp(-0.1*euclidean(each_diphone[1].ravel(),each_vec.ravel()))\n",
    "                        sims.append(sim)\n",
    "                        isincluded.append(1)\n",
    "                        \n",
    "                else:\n",
    "                    if each_diphone[0][0] in training_dict_uni.keys() and each_diphone[0][1] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[:512]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            d=euclidean(current_test_uni,current_e_uni)\n",
    "                            sims.append(np.exp(-0.1*d))\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][1]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[512:]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            sims.append(np.exp(-0.1*euclidean(current_test_uni,current_e_uni)))\n",
    "                    elif each_diphone[0][0] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[:512]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            d=euclidean(current_test_uni,current_e_uni)\n",
    "                            sims.append(np.exp(-0.1*d))\n",
    "                    elif each_diphone[0][1] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][1]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[512:]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            sims.append(np.exp(-0.1*euclidean(current_test_uni,current_e_uni)))\n",
    "                    else:\n",
    "                        sims.append(0)\n",
    "                    isincluded.append(0)\n",
    "                sim_max.append(np.max(sims))\n",
    "                sim_std.append(np.std(sims))\n",
    "                sim_mean.append(np.mean(sims))\n",
    "                    \n",
    "        \n",
    "        sim_mean_max=np.mean(sim_max)\n",
    "        sim_mean_std=np.mean(sim_std)\n",
    "        sim_mean_mean=np.mean(sim_mean)\n",
    "        \n",
    "        X_list.append(X_)\n",
    "        Y_list.append(Y_)\n",
    "        wav2vec_acc_list.append(np.count_nonzero(phoneme_error)/len(phoneme_error))\n",
    "        confusion_matrix_list.append(confusion_matrix)\n",
    "        sim_mean_max_list.append(sim_mean_max)\n",
    "        sim_mean_std_list.append(sim_mean_std)\n",
    "        sim_mean_mean_list.append(sim_mean_mean)\n",
    "        isincluded_list.append(np.count_nonzero(isincluded)/len(isincluded))\n",
    "        diphone_count.append(len(isincluded))\n",
    "        \n",
    "    return sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count,X_list,Y_list,wav2vec_acc_list, confusion_matrix_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
