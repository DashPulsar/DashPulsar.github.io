{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-lv-60-espeak-cv-ft and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\BayesPCN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-large-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from phonecodes import phonecodes\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import Iterable, List\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from torch.nn import Transformer\n",
    "from torch import Tensor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "import tqdm\n",
    "import librosa\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import textgrid\n",
    "from scipy.spatial.distance import euclidean\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import jiwer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "from phonemizer.backend.espeak.wrapper import EspeakWrapper\n",
    "import soundfile as sf\n",
    "\n",
    "_ESPEAK_LIBRARY = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"\n",
    "EspeakWrapper.set_library(_ESPEAK_LIBRARY)\n",
    "processor_P = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model_P = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arpabet_to_ipa={\n",
    "    '<pad>': '<pad>',\n",
    "    '<unk>': '<unk>',\n",
    "    '<s>': '<s>',\n",
    "    '</s>': '</s>',\n",
    "    'AA0': 'ɑ',\n",
    "    'AA1': 'ɑ',#ː\n",
    "    'AA2': 'ɑ',\n",
    "    'AE0': 'æ',\n",
    "    'AE1': 'æ',\n",
    "    'AE2': 'æ',\n",
    "    'AH0': 'ə',\n",
    "    'AH1': 'ʌ',\n",
    "    'AH2': 'ʌ',\n",
    "    'AO0': 'ɔ',\n",
    "    'AO1': 'ɔ',#ː\n",
    "    'AO2': 'ɔ',\n",
    "    'AW0': 'aʊ',\n",
    "    'AW1': 'aʊ',\n",
    "    'AW2': 'aʊ',\n",
    "    'AY0': 'aɪ',\n",
    "    'AY1': 'aɪ',\n",
    "    'AY2': 'aɪ',\n",
    "    'B': 'b',\n",
    "    'CH': 'tʃ',\n",
    "    'D': 'd',\n",
    "    'DH': 'ð',\n",
    "    'EH0': 'ɛ',\n",
    "    'EH1': 'ɛ',\n",
    "    'EH2': 'ɛ',\n",
    "    'ER0': 'ɚ',\n",
    "    'ER1': 'ɚ',\n",
    "    'ER2': 'ɚ',\n",
    "    'EY0': 'eɪ',\n",
    "    'EY1': 'eɪ',\n",
    "    'EY2': 'eɪ',\n",
    "    'F': 'f',\n",
    "    'G': 'ɡ',\n",
    "    'HH': 'h',\n",
    "    'IH0': 'ɪ',\n",
    "    'IH1': 'ɪ',\n",
    "    'IH2': 'ɪ',\n",
    "    'IY0': 'i',\n",
    "    'IY1': 'i',#ː\n",
    "    'IY2': 'i',\n",
    "    'JH': 'dʒ',\n",
    "    'K': 'k',\n",
    "    'L': 'l',\n",
    "    'M': 'm',\n",
    "    'N': 'n',\n",
    "    'NG': 'ŋ',\n",
    "    'OW0': 'oʊ',\n",
    "    'OW1': 'oʊ',\n",
    "    'OW2': 'oʊ',\n",
    "    'OY0': 'ɔɪ',\n",
    "    'OY1': 'ɔɪ',\n",
    "    'OY2': 'ɔɪ',\n",
    "    'P': 'p',\n",
    "    'R': 'ɹ',\n",
    "    'S': 's',\n",
    "    'SH': 'ʃ',\n",
    "    'T': 't',\n",
    "    'TH': 'θ',\n",
    "    'UH0': 'ʊ',\n",
    "    'UH1': 'ʊ',\n",
    "    'UH2': 'ʊ',\n",
    "#    'UW': 'uː',\n",
    "    'UW0': 'u',\n",
    "    'UW1': 'u',#ː\n",
    "    'UW2': 'u',\n",
    "    'V': 'v',\n",
    "    'W': 'w',\n",
    "    'Y': 'j',\n",
    "    'Z': 'z',\n",
    "    'ZH': 'ʒ',\n",
    "}\n",
    "\n",
    "def get_pathset(paths):\n",
    "    return [os.path.join(dir, each_file) for dir, mid, files in os.walk(paths) for each_file in files if each_file.endswith(\".wav\")]\n",
    "\n",
    "def CTC_index(processor,outind):\n",
    "    meaningful_ids = []\n",
    "    meaningful_indices = []\n",
    "    previous_id = -1  \n",
    "    blank_token_id = processor.tokenizer.pad_token_id  \n",
    "    for i, token_id in enumerate(outind[0]):  \n",
    "        if token_id != previous_id and token_id != blank_token_id:\n",
    "            meaningful_ids.append(token_id.item())  \n",
    "            meaningful_indices.append(i)  \n",
    "        previous_id = token_id\n",
    "    \n",
    "    return meaningful_indices\n",
    "\n",
    "def get_training_paths(TrainingTalkerID,all_path):\n",
    "    path_list=[]\n",
    "    TalkerID=[]\n",
    "    for each_ID in TrainingTalkerID.split(\", \"):\n",
    "        if each_ID[:3]==\"CMN\":\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_CMN_ENG_HT1\")\n",
    "        else:\n",
    "            TalkerID.append(f\"ALL_{each_ID[-3:]}_M_ENG_ENG_HT1\")\n",
    "    \n",
    "    for each_path in TalkerID:\n",
    "        for i in all_path:\n",
    "            if each_path in i:\n",
    "                path_list.append(i)\n",
    "                break\n",
    "    \n",
    "    return path_list\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)       \n",
    "    aligned_seq1, aligned_seq2 = [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')  \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1]\n",
    "\n",
    "\n",
    "def get_phonemes(word):\n",
    "    cmu_dict = cmudict.dict()\n",
    "    word = word.lower()\n",
    "    return cmu_dict.get(word, [\"<unk>\"])\n",
    "\n",
    "def align_sequences_with_index(seq1, seq2, index_list):\n",
    "    len1, len2 = len(seq1), len(seq2)\n",
    "    dp = np.zeros((len1 + 1, len2 + 1), dtype=int)\n",
    "\n",
    "\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,   \n",
    "                           dp[i][j - 1] + 1,   \n",
    "                           dp[i - 1][j - 1] + cost)\n",
    "\n",
    "\n",
    "    aligned_seq1, aligned_seq2, aligned_index_list = [], [], []\n",
    "    i, j = len1, len2\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append('<pad>')\n",
    "            aligned_index_list.append(-1) \n",
    "            #aligned_index_list.append(index_list[j - 1] + 1) \n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            aligned_seq1.append('<pad>')\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(-1)\n",
    "            #aligned_index_list.append(index_list[j - 1]) \n",
    "            j -= 1\n",
    "        else:\n",
    "            aligned_seq1.append(seq1[i - 1])\n",
    "            aligned_seq2.append(seq2[j - 1])\n",
    "            aligned_index_list.append(index_list[j - 1])  \n",
    "            i -= 1\n",
    "            j -= 1\n",
    "    return aligned_seq1[::-1], aligned_seq2[::-1], aligned_index_list[::-1]\n",
    "\n",
    "    \n",
    "def filter_none(aligned_seq1, aligned_index):\n",
    "    aligned_index_c = []\n",
    "    aligned_seq1_c = []\n",
    "    if len(aligned_seq1)==len(aligned_index):\n",
    "        for i in range(len(aligned_seq1)):\n",
    "            if aligned_index[i]!=-1:\n",
    "                aligned_index_c.append(aligned_index[i])\n",
    "                aligned_seq1_c.append(aligned_seq1[i])\n",
    "\n",
    "    return aligned_seq1_c, aligned_index_c\n",
    "    \n",
    "\n",
    "def build_exposure_set(paths, native_dict, set_list, model,processor):\n",
    "    diphone_dict={}\n",
    "    uniphone_dict={}\n",
    "    english_phonemes = ['<pad>', '<unk>', '<s>', '</s>', 'ɑ', 'æ', 'ə', 'ʌ', 'ɔ', 'aʊ', 'aɪ', 'b', 'tʃ', 'd', 'ð', 'ɛ', 'ɚ', 'eɪ', 'f',\n",
    " 'g', 'h', 'ɪ', 'i', 'dʒ', 'k', 'l', 'm', 'n', 'ŋ', 'oʊ', 'ɔɪ', 'p', 'ɹ', 's', 'ʃ', 't', 'θ', 'ʊ', 'u', 'v', 'w',\n",
    " 'j', 'z', 'ʒ']#['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    english_phoneme_dict.values()\n",
    "    for each_sentence in paths:\n",
    "        tg = textgrid.TextGrid.fromFile(each_sentence[:-3]+\"TextGrid\")\n",
    "        tg_sentence = [i for i in tg[0] if i.mark!=\"\"]\n",
    "        tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "        tg_sentence = [each for _,each in enumerate(tg_sentence) if _ in set_list]\n",
    "        '''sentence16_end_time=tg_sentence[15].maxTime\n",
    "        tg_sentence = [i for i in tg_sentence if i.maxTime<=sentence16_end_time]\n",
    "        tg_word = [i for i in tg_word if i.maxTime<=sentence16_end_time]'''\n",
    "        \n",
    "        wave, sr = librosa.load(each_sentence)\n",
    "        wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "        #wave_res = wave_res[:int(sentence16_end_time*16000)]\n",
    "        for each_tg in tg_sentence:\n",
    "            #each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_tg.minTime<=i.minTime and each_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "            start=round(each_tg.minTime*16000)\n",
    "            end=round(each_tg.maxTime*16000)\n",
    "            input=processor(wave_res[start:end],sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "            input=input.to(device)\n",
    "            model.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder1=model(input).logits\n",
    "            selected=out_encoder1\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #outind=torch.argmax(out_encoder1,dim=-1).cpu().numpy()\n",
    "            transcription = processor.batch_decode(outind)[0].split(\" \")\n",
    "            phonemeindex = CTC_index(processor,outind)\n",
    "            out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().detach().numpy()\n",
    "            '''aligned_seq1, aligned_seq2 ,phonemeindex= align_sequences_with_index(each_phonemes,transcription,phonemeindex)\n",
    "            if phonemeindex[0]>phonemeindex[1]:\n",
    "                phonemeindex[0]=0\n",
    "            elif phonemeindex[0]==phonemeindex[1] and phonemeindex[1]>phonemeindex[2]:\n",
    "                phonemeindex[0],phonemeindex[1]=0,1\n",
    "            #aligned_seq1, aligned_seq2 = align_sequences(each_phonemes,transcription)\n",
    "            if not len(aligned_seq1)==len(aligned_seq2)==len(phonemeindex):\n",
    "                print(len(aligned_seq1),len(aligned_seq2),len(phonemeindex))\n",
    "                print(aligned_seq1,\"\\n\",aligned_seq2,\"\\n\",phonemeindex)\n",
    "                raise IndexError(\"length unmatch\")'''\n",
    "            \n",
    "            for i in range(len(transcription)-1):#aligned_seq1transcription\n",
    "                key = (transcription[i], transcription[i + 1])\n",
    "                #key = transcription[i] + transcription[i + 1]\n",
    "                if key not in diphone_dict:\n",
    "                    diphone_dict[key] = []\n",
    "                #try:\n",
    "                diphone_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))\n",
    " \n",
    "                #except:\n",
    "                    #print(each_tg)\n",
    "                    #print(aligned_seq1)\n",
    "                    #print(aligned_seq2)\n",
    "                    #print(transcription)\n",
    "                    #print(phonemeindex)\n",
    "                    #print(key)\n",
    "                    #out_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i]])))\n",
    "            for i in range(len(transcription)):\n",
    "                if transcription[i] not in uniphone_dict:\n",
    "                    uniphone_dict[transcription[i]]=[]\n",
    "                uniphone_dict[transcription[i]].append(out_FE[phonemeindex[i]])\n",
    "            '''for i in range(len(transcription)-1):\n",
    "                key = transcription[i] + transcription[i + 1]\n",
    "                if key not in native_dict:\n",
    "                    native_dict[key] = []\n",
    "                native_dict[key].append(np.vstack((out_FE[phonemeindex[i]], out_FE[phonemeindex[i + 1]])))'''\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.cuda.empty_cache()\n",
    "    return diphone_dict,uniphone_dict#native_dict\n",
    "    #'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\ALL_032_M_CMN_ENG_HT1.wav'\n",
    "    \n",
    "    \n",
    "def get_test_list(file_path,key_word,sentenceID,model,processor):\n",
    "    english_phonemes = ['<pad>', '<unk>', '<s>', '</s>', 'ɑ', 'æ', 'ə', 'ʌ', 'ɔ', 'aʊ', 'aɪ', 'b', 'tʃ', 'd', 'ð', 'ɛ', 'ɚ', 'eɪ', 'f',\n",
    " 'g', 'h', 'ɪ', 'i', 'dʒ', 'k', 'l', 'm', 'n', 'ŋ', 'oʊ', 'ɔɪ', 'p', 'ɹ', 's', 'ʃ', 't', 'θ', 'ʊ', 'u', 'v', 'w',\n",
    " 'j', 'z', 'ʒ']#['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    sentenceID=int(sentenceID[-3:])-1\n",
    "    #file_path= f'..\\\\data\\\\raw\\\\ALL_CMN_ENG_HT1\\\\{file_path[:-5]}.wav'\n",
    "    \n",
    "    tg = textgrid.TextGrid.fromFile(file_path[:-3]+\"TextGrid\")\n",
    "    tg_sentence=[]\n",
    "    for i in tg[0][1:]:\n",
    "        if i.mark==\"\":\n",
    "            tg_sentence[-1].maxTime=i.maxTime\n",
    "        else:\n",
    "            tg_sentence.append(i)\n",
    "    tg_sentence = tg_sentence[sentenceID]\n",
    "    \n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    \n",
    "    wave, sr = librosa.load(file_path)\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    \n",
    "\n",
    "    for each_word_tg in tg_word:\n",
    "        if each_word_tg.minTime >= tg_sentence.minTime and each_word_tg.maxTime <= tg_sentence.maxTime:\n",
    "            #print(each_word_tg.mark.lower(),key_word)\n",
    "            if each_word_tg.mark.lower()==key_word:\n",
    "                start=each_word_tg.minTime\n",
    "                end=each_word_tg.maxTime\n",
    "                break\n",
    "                #print(\"start:\",start,\"end:\",end)\n",
    "    #word_length=len(wave_res)/16000\n",
    "    out_list=[]\n",
    "    #each_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if each_word_tg.minTime<=i.minTime and each_word_tg.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "    sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "    word_cut_start=start-tg_sentence.minTime\n",
    "    word_cut_end=end-tg_sentence.minTime\n",
    "    \n",
    "    input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_encoder=model(input.to(device)).logits\n",
    "        out_FE=model.wav2vec2.feature_extractor(input)[0].transpose(1,0).cpu().numpy()\n",
    "    \n",
    "    word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "    word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "    \n",
    "    selected=out_encoder[:,word_start:word_end,:]\n",
    "    mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "    mask[list(english_phoneme_dict.values())] = False\n",
    "    selected[:, :, mask] = 0\n",
    "    outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "    phonemeindex = CTC_index(processor,outind)\n",
    "    transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "    standrad_phonemes=[arpabet_to_ipa[i] for i in get_phonemes(key_word)[0]]\n",
    "    aligned_seq1, aligned_seq2, phonemeindex_= align_sequences_with_index(standrad_phonemes,transcription,phonemeindex)\n",
    "    aligned_seq1 ,phonemeindex_ =  filter_none(aligned_seq1, phonemeindex_)\n",
    "    \n",
    "    if len(phonemeindex_)<2:\n",
    "        each_FE = out_FE[word_start:,:]\n",
    "        '''each_FE = out_FE[word_start:,:]\n",
    "        selected=out_encoder[:,word_start:,:]\n",
    "        mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "        mask[list(english_phoneme_dict.values())] = False\n",
    "        selected[:, :, mask] = 0\n",
    "        outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "        phonemeindex = CTC_index(processor,outind)\n",
    "        transcription = processor_P.batch_decode(outind)[0].split(\" \")'''\n",
    "        \n",
    "        diphone_key = (aligned_seq1[0])\n",
    "        out_list.append((diphone_key, each_FE[phonemeindex_[0]]))#np.vstack((each_FE[phonemeindex[0]], each_FE[phonemeindex[0]]))))\n",
    "\n",
    "    else:\n",
    "        each_FE = out_FE[word_start:word_end,:]\n",
    "        for i in range(len(aligned_seq1)-1):\n",
    "            diphone_key = (aligned_seq1[i] , aligned_seq1[i + 1])\n",
    "        out_list.append((diphone_key, np.vstack((each_FE[phonemeindex_[i]], each_FE[phonemeindex_[i + 1]]))))\n",
    "    torch.cuda.empty_cache()\n",
    "    return out_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def sim_measure(df, model, processor):\n",
    "    sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count =[], [], [], [], []\n",
    "    confusion_matrix_list, wav2vec_acc_list=[],[]\n",
    "    X_list=[]\n",
    "    Y_list=[]\n",
    "    english_phonemes = ['<pad>', '<unk>', '<s>', '</s>', 'ɑ', 'æ', 'ə', 'ʌ', 'ɔ', 'aʊ', 'aɪ', 'b', 'tʃ', 'd', 'ð', 'ɛ', 'ɚ', 'eɪ', 'f',\n",
    " 'g', 'h', 'ɪ', 'i', 'dʒ', 'k', 'l', 'm', 'n', 'ŋ', 'oʊ', 'ɔɪ', 'p', 'ɹ', 's', 'ʃ', 't', 'θ', 'ʊ', 'u', 'v', 'w',\n",
    " 'j', 'z', 'ʒ']#['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    \n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    \n",
    "    phoneme_vocab = {phoneme: idx for idx, phoneme in enumerate(english_phonemes)}\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_set_dict={}\n",
    "    test_word_dict={}\n",
    "    test_matrix_dict={}\n",
    "    for each_ in tqdm.tqdm(df.values):\n",
    "        filename_loc=df.columns.get_loc(\"Filename\")\n",
    "        keyword_loc=df.columns.get_loc(\"Keyword\")\n",
    "        training_talker_loc=df.columns.get_loc(\"TrainingTalkerID\")\n",
    "        \n",
    "        all_path=get_pathset(r\"..\\data\\raw\")\n",
    "        #all_ENG_ENG_pathset=[s.replace(\"raw_L1\", \"raw\") for s in get_pathset(r\"..\\data\\raw_L1\")]\n",
    "        \n",
    "        set1_list=[0,1,2,3,4,5,6,7,8,9,10,12,13,14,15,16]\n",
    "        set2_list=[17,18,19,20,21,22,24,25,26,27,28,29,30,31,37,40]\n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] == \"set2,set1\":\n",
    "            train_set=set2_list\n",
    "            test_set=set1_list\n",
    "        else:\n",
    "            train_set=set1_list\n",
    "            test_set=set2_list\n",
    "        \n",
    "        #print(each_[filename_loc])\n",
    "        test_file = [each for each in all_path if os.path.split(each_[filename_loc])[-1][:-5] in each]\n",
    "        #print(test_file)\n",
    "        key_word = each_[keyword_loc] #string\n",
    "        TrainingTalkerID = each_[training_talker_loc] #list of string\n",
    "        sentenceID = each_[df.columns.get_loc(\"SentenceID\")]\n",
    "        training_files_path=get_training_paths(TrainingTalkerID,all_path)\n",
    "        \n",
    "        #if training_files_path[0] in all_ENG_ENG_pathset:\n",
    "        #    training_dict=copy.deepcopy(all_eng_dict)\n",
    "        #else:\n",
    "        \n",
    "        if TrainingTalkerID not in train_set_dict:\n",
    "            train_set_dict[TrainingTalkerID]={}\n",
    "            \n",
    "        if each_[df.columns.get_loc(\"TrainingTestSet\")] not in train_set_dict[TrainingTalkerID]:\n",
    "            training_dict_di, training_dict_uni=build_exposure_set(training_files_path, {}, train_set, model, processor)#copy.deepcopy(all_eng_dict)\n",
    "            train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]=(copy.deepcopy(training_dict_di),copy.deepcopy(training_dict_uni))\n",
    "        else:\n",
    "            training_dict_di, training_dict_uni=train_set_dict[TrainingTalkerID][each_[df.columns.get_loc(\"TrainingTestSet\")]]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if test_file[0] not in test_word_dict:\n",
    "            test_word_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_word_dict[test_file[0]]:\n",
    "            test_word_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_word_dict[test_file[0]][sentenceID]:\n",
    "            test_list = get_test_list(test_file[0], key_word, sentenceID, model, processor)\n",
    "            test_word_dict[test_file[0]][sentenceID][key_word]=copy.deepcopy(test_list)\n",
    "        else:\n",
    "            test_list=test_word_dict[test_file[0]][sentenceID][key_word]\n",
    "        \n",
    "        if test_file[0] not in test_matrix_dict:\n",
    "            test_matrix_dict[test_file[0]]={}\n",
    "        if sentenceID not in test_matrix_dict[test_file[0]]:\n",
    "            test_matrix_dict[test_file[0]][sentenceID]={}\n",
    "        if key_word not in test_matrix_dict[test_file[0]][sentenceID]:\n",
    "            \n",
    "            tg = textgrid.TextGrid.fromFile(test_file[0][:-3]+\"TextGrid\")\n",
    "            tg_sentence = [i for i in tg[0] if i.mark!=\"\"][int(sentenceID[-3:])-1]\n",
    "            tg_word = [i for i in tg[1] if tg_sentence.minTime<=i.minTime and tg_sentence.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark.lower()==key_word][0]\n",
    "            \n",
    "            each_word_phonemes =[arpabet_to_ipa[i.mark] for i in tg[-1] if tg_word.minTime<=i.minTime and tg_word.maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "            \n",
    "            sentence_total_length=tg_sentence.maxTime-tg_sentence.minTime\n",
    "            word_cut_start=tg_word.minTime-tg_sentence.minTime\n",
    "            word_cut_end=tg_word.maxTime-tg_sentence.minTime\n",
    "            wave, sr = librosa.load(test_file[0])\n",
    "            wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "            input=processor(wave_res[int(tg_sentence.minTime*16000):round(tg_sentence.maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "            with torch.no_grad():\n",
    "                out_encoder=model(input.to(device)).logits\n",
    "                \n",
    "            word_start=round(out_encoder.shape[1]*word_cut_start/sentence_total_length)\n",
    "            word_end=round(out_encoder.shape[1]*word_cut_end/sentence_total_length)\n",
    "            selected=out_encoder[:,word_start:word_end,:]\n",
    "            mask = np.ones(selected.shape[-1], dtype=bool)\n",
    "            mask[list(english_phoneme_dict.values())] = False\n",
    "            selected[:, :, mask] = 0\n",
    "            outind=torch.argmax(selected,dim=-1).cpu().numpy()\n",
    "            #phonemeindex = CTC_index(processor,outind)\n",
    "            transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "            X_=each_word_phonemes\n",
    "            Y_=transcription\n",
    "            #print(each_word_phonemes,\"\\n\",transcription)\n",
    "            aligned_seq1, aligned_seq2 = align_sequences(each_word_phonemes,transcription)\n",
    "            \n",
    "            N=len(list(phoneme_vocab.keys()))\n",
    "            confusion_matrix = np.zeros((N, N), dtype=int)\n",
    "            #print(confusion_matrix.shape)\n",
    "            for true_phoneme, predicted_phoneme in zip(aligned_seq1, aligned_seq2):\n",
    "                if predicted_phoneme in english_phonemes[:4]:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[true_phoneme]\n",
    "                else:\n",
    "                    true_idx = phoneme_vocab[true_phoneme]\n",
    "                    predicted_idx = phoneme_vocab[predicted_phoneme]\n",
    "                #print(true_idx, predicted_idx)\n",
    "                confusion_matrix[predicted_idx,true_idx] += 1\n",
    "            \n",
    "            \n",
    "            phoneme_error = [1 if aligned_seq1[_]==aligned_seq2[_] else 0 for _ in range(len(aligned_seq1))]\n",
    "            #print(np.array(list(english_phoneme_dict.values())))\n",
    "            #confusion_matrix = #out_encoder[:,word_start:word_end,list(english_phoneme_dict.values())].cpu().numpy()\n",
    "            test_matrix_dict[test_file[0]][sentenceID][key_word]=(X_,Y_,phoneme_error, confusion_matrix)\n",
    "        else:\n",
    "            X_=test_matrix_dict[test_file[0]][sentenceID][key_word][0]\n",
    "            Y_=test_matrix_dict[test_file[0]][sentenceID][key_word][1]\n",
    "            phoneme_error = test_matrix_dict[test_file[0]][sentenceID][key_word][2]\n",
    "            confusion_matrix = copy.deepcopy(test_matrix_dict[test_file[0]][sentenceID][key_word][3])\n",
    "        \n",
    "        # word level, list\n",
    "        sim_max=[]\n",
    "        sim_std=[]\n",
    "        sim_mean=[]\n",
    "        isincluded=[]\n",
    "        #sim_count=[]\n",
    "        for _, each_diphone in enumerate(test_list):\n",
    "            sims=[]\n",
    "            if len(each_diphone[0])==1:\n",
    "                if each_diphone[0][0] in training_dict_uni.keys():\n",
    "                    for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                        current_test_uni=each_diphone[1].ravel()\n",
    "                        current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                        current_e_uni=each_vec.ravel()\n",
    "                        current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                        sim=np.exp(-0.1*euclidean(current_test_uni,current_e_uni))\n",
    "                        sims.append(sim)\n",
    "                        isincluded.append(0)\n",
    "                        \n",
    "                else:\n",
    "                    sims.append(0)\n",
    "                    isincluded.append(0)\n",
    "                    warnings.warn(\"Uniphone unmatch: \"+each_diphone[0][0], UserWarning)\n",
    "                    #raise Warning(\"Uniphone unmatch\"+each_diphone[0][0])\n",
    "                \n",
    "                sim_max.append(np.max(sims))\n",
    "                sim_std.append(np.std(sims))\n",
    "                sim_mean.append(np.mean(sims))\n",
    "            else:\n",
    "                if each_diphone[0] in training_dict_di.keys():\n",
    "                    for each_vec in training_dict_di[each_diphone[0]]:\n",
    "                        sim=np.exp(-0.1*euclidean(each_diphone[1].ravel(),each_vec.ravel()))\n",
    "                        sims.append(sim)\n",
    "                        isincluded.append(1)\n",
    "                        \n",
    "                else:\n",
    "                    if each_diphone[0][0] in training_dict_uni.keys() and each_diphone[0][1] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[:512]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            d=euclidean(current_test_uni,current_e_uni)\n",
    "                            sims.append(np.exp(-0.1*d))\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][1]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[512:]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            sims.append(np.exp(-0.1*euclidean(current_test_uni,current_e_uni)))\n",
    "                    elif each_diphone[0][0] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][0]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[:512]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            d=euclidean(current_test_uni,current_e_uni)\n",
    "                            sims.append(np.exp(-0.1*d))\n",
    "                    elif each_diphone[0][1] in training_dict_uni.keys():\n",
    "                        for each_vec in training_dict_uni[each_diphone[0][1]]:\n",
    "                            current_test_uni=each_diphone[1].ravel()[512:]\n",
    "                            current_test_uni=np.hstack((current_test_uni,current_test_uni))\n",
    "                            current_e_uni=each_vec.ravel()\n",
    "                            current_e_uni=np.hstack((current_e_uni,current_e_uni))\n",
    "                            sims.append(np.exp(-0.1*euclidean(current_test_uni,current_e_uni)))\n",
    "                    else:\n",
    "                        sims.append(0)\n",
    "                    isincluded.append(0)\n",
    "                sim_max.append(np.max(sims))\n",
    "                sim_std.append(np.std(sims))\n",
    "                sim_mean.append(np.mean(sims))\n",
    "                    \n",
    "        \n",
    "        sim_mean_max=np.mean(sim_max)\n",
    "        sim_mean_std=np.mean(sim_std)\n",
    "        sim_mean_mean=np.mean(sim_mean)\n",
    "        \n",
    "        X_list.append(X_)\n",
    "        Y_list.append(Y_)\n",
    "        wav2vec_acc_list.append(np.count_nonzero(phoneme_error)/len(phoneme_error))\n",
    "        confusion_matrix_list.append(confusion_matrix)\n",
    "        sim_mean_max_list.append(sim_mean_max)\n",
    "        sim_mean_std_list.append(sim_mean_std)\n",
    "        sim_mean_mean_list.append(sim_mean_mean)\n",
    "        isincluded_list.append(np.count_nonzero(isincluded)/len(isincluded))\n",
    "        diphone_count.append(len(isincluded))\n",
    "        \n",
    "    return sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count,X_list,Y_list,wav2vec_acc_list, confusion_matrix_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_result_path=r\"..\\data\\test.xlsx\"\n",
    "human_result = pd.read_excel(human_result_path)\n",
    "human_result_1a=human_result[human_result[\"Experiment\"]==\"1a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16477 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1113/16477 [02:29<00:39, 388.20it/s]C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\3825428957.py:146: UserWarning: Uniphone unmatch: ɑ\n",
      "  warnings.warn(\"Uniphone unmatch: \"+each_diphone[0][0], UserWarning)\n",
      "100%|██████████| 16477/16477 [10:01<00:00, 27.41it/s] \n"
     ]
    }
   ],
   "source": [
    "sim_mean_max_list,sim_mean_std_list,sim_mean_mean_list,isincluded_list,diphone_count,X_list,Y_list,wav2vec_acc_list, confusion_matrix_list=sim_measure(human_result_1a,model_P, processor_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"diphone_overlapped_rate\"]=isincluded_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"NumDiphone_word\"]=diphone_count\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"wav2vec_acc\"]=wav2vec_acc_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"x\"]=X_list\n",
      "C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_419952\\1843124913.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  human_result_1a[\"y\"]=Y_list\n"
     ]
    }
   ],
   "source": [
    "human_result_1a[\"sim_mean_max\"]=sim_mean_max_list\n",
    "human_result_1a[\"sim_mean_std\"]=sim_mean_std_list\n",
    "human_result_1a[\"sim_mean_mean\"] = sim_mean_mean_list\n",
    "human_result_1a[\"diphone_overlapped_rate\"]=isincluded_list\n",
    "human_result_1a[\"NumDiphone_word\"]=diphone_count\n",
    "human_result_1a[\"wav2vec_acc\"]=wav2vec_acc_list\n",
    "human_result_1a[\"x\"]=X_list\n",
    "human_result_1a[\"y\"]=Y_list\n",
    "human_result_1a.to_excel('similarities_b.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\data\\\\raw_L1\\\\ALL_ENG_ENG_HT1\\\\ALL_049_F_ENG_ENG_HT1.wav'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_ENG_ENG_path=r\"..\\data\\raw_L1\"\n",
    "ALL_ENG_ENG_pathset=get_pathset(ALL_ENG_ENG_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg = textgrid.TextGrid.fromFile(ALL_ENG_ENG_pathset[0][:-3]+\"TextGrid\")\n",
    "tg_sentence_mark = [i.mark.lower() for i in tg[0] if i.mark!=\"\"]\n",
    "tg_sentence= [i for i in tg[0] if i.mark!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interval(-0.001, 0.042, None)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg[0][0].minTime=-0.001\n",
    "tg[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Interval(0.042, 1.822, A BOY FELL FROM A WINDOW),\n",
       " Interval(1.822, 3.497, THE WIFE HELPED HER HUSBAND),\n",
       " Interval(3.497, 5.287, BIG DOGS CAN BE DANGEROUS),\n",
       " Interval(5.287, 6.755, THE SHOES WERE VERY DIRTY),\n",
       " Interval(6.755, 8.121, THE PLAYER LOST A SHOE),\n",
       " Interval(8.121, 9.375, SOMEBODY STOLE THE MONEY),\n",
       " Interval(9.375, 10.968, THE FIRE WAS VERY HOT),\n",
       " Interval(10.968, 12.616, SHE'S DRINKING FROM HER OWN CUP),\n",
       " Interval(12.616, 14.086, THE PICTURE CAME FROM A BOOK),\n",
       " Interval(14.086, 15.916, THE CAR IS GOING TOO FAST),\n",
       " Interval(15.916, 17.475, THE PAINT DRIPPED ON THE GROUND),\n",
       " Interval(17.475, 19.174, THE TOWEL FELL ON THE FLOOR),\n",
       " Interval(19.174, 20.623, THE FAMILY LIKES FISH),\n",
       " Interval(20.623, 22.084, THE BANANAS ARE TOO RIPE),\n",
       " Interval(22.084, 23.614, HE GREW LOTS OF VEGETABLES),\n",
       " Interval(23.614, 25.122, SHE ARGUES WITH HER SISTER),\n",
       " Interval(25.122, 26.613, THE KITCHEN WINDOW WAS CLEAN),\n",
       " Interval(26.613, 27.845, HE HUNG UP HIS RAINCOAT),\n",
       " Interval(27.845, 29.112, THE MAILMAN BROUGHT A LETTER),\n",
       " Interval(29.112, 30.362, THE MOTHER HEARD THE BABY),\n",
       " Interval(30.362, 32.213, SHE FOUND HER PURSE IN THE TRASH),\n",
       " Interval(32.213, 33.775, THE TABLE HAS THREE LEGS),\n",
       " Interval(33.775, 35.316, THE CHILDREN WAVED AT THE TRAIN),\n",
       " Interval(35.316, 36.639, HER COAT IS ON A CHAIR),\n",
       " Interval(36.639, 38.238, THE GIRL IS FIXING HER DRESS),\n",
       " Interval(38.238, 39.452, IT'S TIME TO GO TO BED),\n",
       " Interval(39.452, 41.008, MOTHER READ THE INSTRUCTIONS),\n",
       " Interval(41.008, 42.469, THE DOG IS EATING SOME MEAT),\n",
       " Interval(42.469, 43.648, FATHER FORGOT THE BREAD),\n",
       " Interval(43.648, 44.983, THE ROAD GOES UP A HILL),\n",
       " Interval(44.983, 46.458, THE PAINTER USES A BRUSH),\n",
       " Interval(46.458, 48.007, THE FAMILY BOUGHT A HOUSE),\n",
       " Interval(48.007, 49.618, SWIMMERS CAN HOLD THEIR BREATH),\n",
       " Interval(49.618, 51.239, SHE CUT THE STEAK WITH HER KNIFE),\n",
       " Interval(51.239, 52.582, THEY'RE PUSHING AN OLD CAR),\n",
       " Interval(52.582, 53.921, THE FOOD IS EXPENSIVE),\n",
       " Interval(53.921, 55.407, THE CHILDREN ARE WALKING HOME),\n",
       " Interval(55.407, 56.856, THEY HAD TWO EMPTY BOTTLES),\n",
       " Interval(56.856, 58.171, MILK COMES IN A CARTON),\n",
       " Interval(58.171, 59.644, THE DOG SLEEPS IN A BASKET),\n",
       " Interval(59.644, 61.266, THE HOUSE HAD NINE BEDROOMS),\n",
       " Interval(61.266, 62.872, THEY'RE SHOPPING FOR SCHOOL CLOTHES),\n",
       " Interval(62.872, 63.978, THEY'RE PLAYING IN THE PARK),\n",
       " Interval(63.978, 65.505, RAIN IS GOOD FOR TREES),\n",
       " Interval(65.505, 67.022, THEY SAT ON A WOODEN BENCH),\n",
       " Interval(67.022, 68.756, THE CHILD DRANK SOME FRESH MILK),\n",
       " Interval(68.756, 70.127, THE BABY SLEPT ALL NIGHT),\n",
       " Interval(70.127, 71.473, THE SALT SHAKER IS EMPTY),\n",
       " Interval(71.473, 72.847, THE POLICEMAN KNOWS THE WAY),\n",
       " Interval(72.847, 74.305, THE BUCKETS FILL UP QUICKLY),\n",
       " Interval(74.305, 75.505, THE BOY IS RUNNING AWAY),\n",
       " Interval(75.505, 76.947, A TOWEL IS NEAR THE SINK),\n",
       " Interval(76.947, 78.335, FLOWERS CAN GROW IN THE POT),\n",
       " Interval(78.335, 79.571, HE'S SKATING WITH HIS FRIEND),\n",
       " Interval(79.571, 81.0, THE JANITOR SWEPT THE FLOOR),\n",
       " Interval(81.0, 82.276, THE LADY WASHED THE SHIRT),\n",
       " Interval(82.276, 83.547, SHE TOOK OFF HER FUR COAT),\n",
       " Interval(83.547, 84.998, THE MATCH BOXES ARE EMPTY),\n",
       " Interval(84.998, 86.495, THE MAN IS PAINTING A SIGN),\n",
       " Interval(86.495, 88.075, THE DOG CAME HOME AT LAST)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tg_sentence=[]\n",
    "for i in tg[0][1:]:\n",
    "    if i.mark==\"\":\n",
    "        tg_sentence[-1].maxTime=copy.deepcopy(i.maxTime)\n",
    "    else:\n",
    "        tg_sentence.append(i)\n",
    "tg_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\Alex\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[['AH0'], ['EY1']],\n",
       " [['B', 'OY1']],\n",
       " [['F', 'EH1', 'L']],\n",
       " [['F', 'R', 'AH1', 'M'], ['F', 'ER0', 'M']],\n",
       " [['AH0'], ['EY1']],\n",
       " [['W', 'IH1', 'N', 'D', 'OW0']]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 初始化字典\n",
    "\n",
    "\n",
    "def get_phonemes(word):\n",
    "    word = word.lower()\n",
    "    return cmu_dict.get(word, [\"<unk>\"])\n",
    "\n",
    "# 测试句子\n",
    "sentence =tg_sentence[0] #\"This is a test sentence\"\n",
    "words = sentence.split()\n",
    "phonemes = in words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[get_phonemes(word) for word in each_tg.mark.split()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AH0',\n",
       " 'B',\n",
       " 'OY1',\n",
       " 'F',\n",
       " 'EH1',\n",
       " 'L',\n",
       " 'F',\n",
       " 'R',\n",
       " 'AH1',\n",
       " 'M',\n",
       " 'AH0',\n",
       " 'W',\n",
       " 'IH1',\n",
       " 'N',\n",
       " 'D',\n",
       " 'OW0']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh=[]\n",
    "\n",
    "for i in phonemes:\n",
    "    \n",
    "    for j in i[0]:\n",
    "        #print(j)\n",
    "        oh.append(j)\n",
    "oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AH0',\n",
       " 'B',\n",
       " 'OY1',\n",
       " 'F',\n",
       " 'EH1',\n",
       " 'L',\n",
       " 'F',\n",
       " 'R',\n",
       " 'AH1',\n",
       " 'M',\n",
       " 'AH0',\n",
       " 'W',\n",
       " 'IH1',\n",
       " 'N',\n",
       " 'D',\n",
       " 'OW0']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arpabet_to_ipa[\n",
    "each_phonemes =[i.mark for i in tg[-1] if tg_sentence[0].minTime<=i.minTime and tg_sentence[0].maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "each_phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D', 'EY1', 'N', 'JH', 'ER0', 'AH0', 'S']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a boy fell from the window\n",
      "['AH0', 'B', 'OY1', 'F', 'EH1', 'L', 'F', 'R', 'AH1', 'M', 'AH0', 'W', 'IH1', 'N', 'D', 'OW0']\n",
      "['AH0', 'B', 'OY1', 'F', 'EH1', 'L', 'F', 'R', 'AH1', 'M', 'DH', 'AH0', 'W', 'IH1', 'N', 'D', 'OW0']\n",
      "['ʌ', 'b', 'ɔɪ', 'f', 'e', 'l', 'f', 'ɹ', 'ʌ', 'm', 'ð', 'ʌ', 'w', 'ɪ', 'n', 'd', 'oʊ']\n",
      "['ʌ', 'b', 'ɔɪ', 'f', 'æ', 'l', 'f', 'ɹ', 'ʌ', 'm', 'ð', 'w', 'ɪ', 'n', 'd', 'oʊ']\n",
      "[3, 10, 13, 31, 34, 40, 46, 48, 50, 53, 56, 61, 63, 67, 70, 73]\n",
      "[3, 10, 13, 31, 34, 40, 46, 48, 50, 53, 56, -1, 61, 63, 67, 70, 73]\n",
      "\n",
      "\n",
      "the wife helped her husband\n",
      "['DH', 'AH0', 'W', 'AY1', 'F', 'HH', 'EH1', 'L', 'P', 'T', 'HH', 'ER1', 'HH', 'AH1', 'Z', 'B', 'AH0', 'N', 'D']\n",
      "['DH', 'AH0', 'W', 'AY1', 'F', 'HH', 'EH1', 'L', 'P', 'T', 'HH', 'ER0', 'HH', 'AH1', 'Z', 'B', 'AH0', 'N', 'D']\n",
      "['ð', 'ʌ', 'w', 'aɪ', 'f', 'h', 'e', 'l', 'p', 't', 'h', 'ɜ', 'h', 'ʌ', 'z', 'b', 'ʌ', 'n', 'd']\n",
      "['ð', 'ʊ', 'w', 'aɪ', 'f', 'h', 'æ', 'l', 'p', 't', 'h', 'ɹ', 'h', 'ʌ', 'z', 'b', 'ɪ', 'n', 'd']\n",
      "[1, 3, 8, 11, 22, 27, 31, 34, 39, 42, 45, 47, 52, 55, 60, 64, 66, 71, 75]\n",
      "[1, 3, 8, 11, 22, 27, 31, 34, 39, 42, 45, 47, 52, 55, 60, 64, 66, 71, 75]\n",
      "\n",
      "\n",
      "big dogs can be dangerous\n",
      "['B', 'IH1', 'G', 'D', 'AA1', 'G', 'Z', 'K', 'AH0', 'N', 'B', 'IY0', 'D', 'EY1', 'N', 'JH', 'ER0', 'AH0', 'S']\n",
      "['B', 'IH1', 'G', 'D', 'AA1', 'G', 'Z', 'K', 'AE1', 'N', 'B', 'IY1', 'D', 'EY1', 'N', 'JH', 'ER0', 'AH0', 'S']\n",
      "['b', 'ɪ', 'ɡ', 'd', 'ɑ', 'ɡ', 'z', 'k', 'æ', 'n', 'b', 'i', 'd', 'eɪ', 'n', 'dʒ', 'ɜ', 'ʌ', 's']\n",
      "['b', 'ɪ', 'ɡ', 'd', 'aʊ', 'ɡ', 'z', 'k', 'æ', 'n', 'b', 'i', 'd', 'eɪ', 'n', 'dʒ', 'ɹ', 'ɹ', 'ɪ', 's']\n",
      "[1, 4, 9, 13, 17, 27, 30, 34, 36, 39, 42, 44, 49, 52, 57, 60, 63, 65, 68, 76]\n",
      "[1, 4, 9, 13, 17, 27, 30, 34, 36, 39, 42, 44, 49, 52, 57, 60, 63, 65, -1, 76]\n",
      "\n",
      "\n",
      "the shoes were very dirty\n",
      "['DH', 'AH0', 'SH', 'UW1', 'Z', 'W', 'ER0', 'V', 'EH1', 'R', 'IY0', 'D', 'ER1', 'T', 'IY0']\n",
      "['DH', 'AH0', 'SH', 'UW1', 'Z', 'W', 'ER0', 'V', 'EH1', 'R', 'IY0', 'D', 'ER1', 'T', 'IY0']\n",
      "['ð', 'ʌ', 'ʃ', 'u', 'z', 'w', 'ɜ', 'v', 'e', 'ɹ', 'i', 'd', 'ɜ', 't', 'i']\n",
      "['ð', 'ɪ', 'ʃ', 'u', 'z', 'w', 'æ', 'v', 'ɹ', 'i', 'd', 'æ', 't', 'i']\n",
      "[2, 4, 9, 15, 23, 27, 29, 35, 43, 45, 51, 54, 61, 63]\n",
      "[2, 4, 9, 15, 23, 27, 29, 35, -1, 43, 45, 51, 54, 61, 63]\n",
      "\n",
      "\n",
      "the player lost a shoe\n",
      "['DH', 'AH0', 'P', 'L', 'EY1', 'ER0', 'L', 'AO1', 'S', 'T', 'AH0', 'SH', 'UW1']\n",
      "['DH', 'AH0', 'P', 'L', 'EY1', 'ER0', 'L', 'AO1', 'S', 'T', 'AH0', 'SH', 'UW1']\n",
      "['ð', 'ʌ', 'p', 'l', 'eɪ', 'ɜ', 'l', 'ɒ', 's', 't', 'ʌ', 'ʃ', 'u']\n",
      "['ð', 'p', 'l', 'eɪ', 'l', 'aʊ', 's', 't', 'ʌ', 'ʃ', 'ʊ']\n",
      "[1, 7, 10, 12, 24, 27, 35, 38, 40, 46, 52]\n",
      "[1, -1, 7, 10, 12, -1, 24, 27, 35, 38, 40, 46, 52]\n",
      "\n",
      "\n",
      "somebody stole the money\n",
      "['S', 'AH1', 'M', 'B', 'AH0', 'D', 'IY0', 'S', 'T', 'OW1', 'L', 'DH', 'AH1', 'M', 'AH1', 'N', 'IY0']\n",
      "['S', 'AH1', 'M', 'B', 'AA2', 'D', 'IY0', 'S', 'T', 'OW1', 'L', 'DH', 'AH0', 'M', 'AH1', 'N', 'IY0']\n",
      "['s', 'ʌ', 'm', 'b', 'ɑ', 'd', 'i', 's', 't', 'oʊ', 'l', 'ð', 'ʌ', 'm', 'ʌ', 'n', 'i']\n",
      "['s', 'ʌ', 'm', 'b', 'ʌ', 'd', 'i', 's', 't', 'oʊ', 'l', 'ð', 'ʌ', 'm', 'ʌ', 'n', 'i']\n",
      "[2, 5, 9, 11, 12, 15, 16, 22, 26, 29, 34, 37, 39, 43, 45, 49, 51]\n",
      "[2, 5, 9, 11, 12, 15, 16, 22, 26, 29, 34, 37, 39, 43, 45, 49, 51]\n",
      "\n",
      "\n",
      "the fire was very hot\n",
      "['DH', 'AH0', 'F', 'AY1', 'ER0', 'W', 'AH0', 'Z', 'V', 'EH1', 'R', 'IY0', 'HH', 'AA1', 'T']\n",
      "['DH', 'AH0', 'F', 'AY1', 'ER0', 'W', 'AA1', 'Z', 'V', 'EH1', 'R', 'IY0', 'HH', 'AA1', 'T']\n",
      "['ð', 'ʌ', 'f', 'aɪ', 'ɜ', 'w', 'ɑ', 'z', 'v', 'e', 'ɹ', 'i', 'h', 'ɑ', 't']\n",
      "['ð', 'ʌ', 'f', 'aɪ', 'w', 'ʌ', 'z', 'v', 'æ', 'ɹ', 'i', 'h', 'æ', 't']\n",
      "[3, 4, 10, 14, 28, 30, 34, 39, 41, 47, 49, 55, 59, 72]\n",
      "[3, 4, 10, 14, -1, 28, 30, 34, 39, 41, 47, 49, 55, 59, 72]\n",
      "\n",
      "\n",
      "she's drinking from her own cup\n",
      "['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'ER0', 'M', 'HH', 'ER1', 'OW1', 'N', 'K', 'AH1', 'P']\n",
      "['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'R', 'AH1', 'M', 'HH', 'ER0', 'OW1', 'N', 'K', 'AH1', 'P']\n",
      "['ʃ', 'i', 'z', 'd', 'ɹ', 'ɪ', 'ŋ', 'k', 'ɪ', 'ŋ', 'f', 'ɹ', 'ʌ', 'm', 'h', 'ɜ', 'oʊ', 'n', 'k', 'ʌ', 'p']\n",
      "['ʃ', 'i', 'z', 'd', 'ɹ', 'ɪ', 'ŋ', 'k', 'ɪ', 'ŋ', 'f', 'ɹ', 'ʌ', 'm', 'h', 'ɹ', 'ɹ', 'oʊ', 'n', 'k', 'ʌ', 'p']\n",
      "[2, 5, 9, 14, 16, 18, 21, 23, 26, 29, 33, 34, 36, 39, 42, 45, 47, 52, 58, 63, 67, 77]\n",
      "[2, 5, 9, 14, 16, 18, 21, 23, 26, 29, 33, 34, 36, 39, 42, 45, -1, 52, 58, 63, 67, 77]\n",
      "\n",
      "\n",
      "the picture came from a book\n",
      "['DH', 'AH0', 'P', 'IH1', 'K', 'CH', 'ER0', 'K', 'EY1', 'M', 'F', 'ER0', 'M', 'AH0', 'B', 'UH1', 'K']\n",
      "['DH', 'AH0', 'P', 'IH1', 'K', 'CH', 'ER0', 'K', 'EY1', 'M', 'F', 'R', 'AH1', 'M', 'AH0', 'B', 'UH1', 'K']\n",
      "['ð', 'ʌ', 'p', 'ɪ', 'k', 'tʃ', 'ɜ', 'k', 'eɪ', 'm', 'f', 'ɹ', 'ʌ', 'm', 'ʌ', 'b', 'ʊ', 'k']\n",
      "['ð', 'ʌ', 'p', 'ɪ', 'k', 'tʃ', 'ɹ', 'k', 'eɪ', 'm', 'f', 'ɹ', 'ʌ', 'm', 'ʌ', 'b', 'ʊ', 'k']\n",
      "[2, 3, 9, 11, 15, 17, 21, 26, 30, 36, 40, 41, 43, 46, 48, 53, 56, 66]\n",
      "[2, 3, 9, 11, 15, 17, 21, 26, 30, 36, 40, 41, 43, 46, 48, 53, 56, 66]\n",
      "\n",
      "\n",
      "the car is going too fast\n",
      "['DH', 'AH0', 'K', 'AA1', 'R', 'IH1', 'Z', 'G', 'OW1', 'IH0', 'NG', 'T', 'UW1', 'F', 'AE1', 'S', 'T']\n",
      "['DH', 'AH0', 'K', 'AA1', 'R', 'IH1', 'Z', 'G', 'OW1', 'IH0', 'NG', 'T', 'UW1', 'F', 'AE1', 'S', 'T']\n",
      "['ð', 'ʌ', 'k', 'ɑ', 'ɹ', 'ɪ', 'z', 'ɡ', 'oʊ', 'ɪ', 'ŋ', 't', 'u', 'f', 'æ', 's', 't']\n",
      "['ð', 'ɪ', 'k', 'aʊ', 'ɹ', 'ɪ', 'z', 'ɡ', 'oʊ', 'ɪ', 'ŋ', 't', 'ʊ', 'f', 'æ', 's', 't']\n",
      "[1, 3, 8, 13, 21, 24, 27, 31, 33, 39, 42, 47, 51, 58, 62, 75, 83]\n",
      "[1, 3, 8, 13, 21, 24, 27, 31, 33, 39, 42, 47, 51, 58, 62, 75, 83]\n",
      "\n",
      "\n",
      "the paint dripped on the ground\n",
      "['DH', 'AH0', 'P', 'EY1', 'N', 'T', 'D', 'R', 'IH1', 'P', 'T', 'AA1', 'N', 'DH', 'AH0', 'G', 'R', 'AW1', 'N', 'D']\n",
      "['DH', 'AH0', 'P', 'EY1', 'N', 'T', 'D', 'R', 'IH1', 'P', 'T', 'AA1', 'N', 'DH', 'AH0', 'G', 'R', 'AW1', 'N', 'D']\n",
      "['ð', 'ʌ', 'p', 'eɪ', 'n', 't', 'd', 'ɹ', 'ɪ', 'p', 't', 'ɑ', 'n', 'ð', 'ʌ', 'ɡ', 'ɹ', 'aʊ', 'n', 'd']\n",
      "['ð', 'ʌ', 'p', 'eɪ', 'n', 't', 'd', 'ɹ', 'ɪ', 'p', 't', 'aʊ', 'n', 'ð', 'ɪ', 'ɡ', 'ɹ', 'aʊ', 'n', 'd']\n",
      "[1, 2, 8, 12, 18, 20, 26, 27, 30, 35, 39, 43, 48, 49, 51, 54, 56, 59, 69, 73]\n",
      "[1, 2, 8, 12, 18, 20, 26, 27, 30, 35, 39, 43, 48, 49, 51, 54, 56, 59, 69, 73]\n",
      "\n",
      "\n",
      "the towel fell on the floor\n",
      "['DH', 'AH0', 'T', 'AW1', 'L', 'F', 'EH1', 'L', 'AA1', 'N', 'DH', 'AH0', 'F', 'L', 'AO1', 'R']\n",
      "['DH', 'AH0', 'T', 'AW1', 'AH0', 'L', 'F', 'EH1', 'L', 'AA1', 'N', 'DH', 'AH0', 'F', 'L', 'AO1', 'R']\n",
      "['ð', 'ʌ', 't', 'aʊ', 'ʌ', 'l', 'f', 'e', 'l', 'ɑ', 'n', 'ð', 'ʌ', 'f', 'l', 'ɒ', 'ɹ']\n",
      "['ð', 'ɪ', 't', 'aʊ', 'l', 'f', 'æ', 'l', 'ʌ', 'n', 'ð', 'ʌ', 'f', 'l', 'oʊ']\n",
      "[2, 3, 8, 13, 25, 33, 36, 43, 50, 53, 55, 57, 61, 65, 68]\n",
      "[2, 3, 8, 13, -1, 25, 33, 36, 43, 50, 53, 55, 57, 61, 65, 68, -1]\n",
      "\n",
      "\n",
      "the family likes fish\n",
      "['DH', 'AH0', 'F', 'AE1', 'M', 'L', 'IY0', 'L', 'AY1', 'K', 'S', 'F', 'IH1', 'SH']\n",
      "['DH', 'AH0', 'F', 'AE1', 'M', 'AH0', 'L', 'IY0', 'L', 'AY1', 'K', 'S', 'F', 'IH1', 'SH']\n",
      "['ð', 'ʌ', 'f', 'æ', 'm', 'ʌ', 'l', 'i', 'l', 'aɪ', 'k', 's', 'f', 'ɪ', 'ʃ']\n",
      "['ð', 'ʌ', 'f', 'æ', 'm', 'l', 'i', 'l', 'aɪ', 'k', 's', 'f', 'ɪ', 'ʃ']\n",
      "[2, 3, 9, 13, 19, 22, 24, 30, 33, 39, 42, 48, 51, 60]\n",
      "[2, 3, 9, 13, 19, -1, 22, 24, 30, 33, 39, 42, 48, 51, 60]\n",
      "\n",
      "\n",
      "the bananas are too ripe\n",
      "['DH', 'AH0', 'B', 'AH0', 'N', 'AE1', 'N', 'AH0', 'Z', 'ER0', 'T', 'UW1', 'R', 'AY1', 'P']\n",
      "['DH', 'AH0', 'B', 'AH0', 'N', 'AE1', 'N', 'AH0', 'Z', 'AA1', 'R', 'T', 'UW1', 'R', 'AY1', 'P']\n",
      "['ð', 'ʌ', 'b', 'ʌ', 'n', 'æ', 'n', 'ʌ', 'z', 'ɑ', 'ɹ', 't', 'u', 'ɹ', 'aɪ', 'p']\n",
      "['ð', 'ʌ', 'b', 'ʌ', 'n', 'æ', 'n', 'ʌ', 'z', 'æ', 't', 'ʊ', 'ɹ', 'aɪ', 'p']\n",
      "[1, 2, 6, 8, 12, 15, 20, 22, 28, 34, 41, 46, 53, 56, 68]\n",
      "[1, 2, 6, 8, 12, 15, 20, 22, 28, 34, -1, 41, 46, 53, 56, 68]\n",
      "\n",
      "\n",
      "he grew lots of vegetables\n",
      "['HH', 'IY1', 'G', 'R', 'UW1', 'L', 'AA1', 'T', 'S', 'AH0', 'V', 'V', 'EH1', 'JH', 'T', 'AH0', 'B', 'AH0', 'L', 'Z']\n",
      "['HH', 'IY1', 'G', 'R', 'UW1', 'L', 'AA1', 'T', 'S', 'AH1', 'V', 'V', 'EH1', 'JH', 'T', 'AH0', 'B', 'AH0', 'L', 'Z']\n",
      "['h', 'i', 'ɡ', 'ɹ', 'u', 'l', 'ɑ', 't', 's', 'ʌ', 'v', 'v', 'e', 'dʒ', 't', 'ʌ', 'b', 'ʌ', 'l', 'z']\n",
      "['h', 'ɪ', 'ɡ', 'ɹ', 'u', 'l', 'æ', 't', 's', 'ʌ', 'v', 'æ', 'dʒ', 't', 'ɪ', 'b', 'oʊ', 'z']\n",
      "[1, 3, 9, 11, 13, 20, 22, 28, 29, 32, 37, 39, 44, 49, 51, 54, 56, 64]\n",
      "[1, 3, 9, 11, 13, 20, 22, 28, 29, 32, 37, 39, -1, 44, 49, 51, 54, 56, -1, 64]\n",
      "\n",
      "\n",
      "she argues with her sister\n",
      "['SH', 'IY1', 'AA1', 'R', 'G', 'Y', 'UW0', 'Z', 'W', 'AH0', 'DH', 'HH', 'ER1', 'S', 'IH1', 'S', 'T', 'ER0']\n",
      "['SH', 'IY1', 'AA1', 'R', 'G', 'Y', 'UW0', 'Z', 'W', 'IH1', 'DH', 'HH', 'ER0', 'S', 'IH1', 'S', 'T', 'ER0']\n",
      "['ʃ', 'i', 'ɑ', 'ɹ', 'ɡ', 'j', 'u', 'z', 'w', 'ɪ', 'ð', 'h', 'ɜ', 's', 'ɪ', 's', 't', 'ɜ']\n",
      "['ʃ', 'i', 'æ', 'ɡ', 'j', 'u', 'z', 'w', 'ɪ', 'ð', 'h', 'ɹ', 's', 'ɪ', 's', 't', 'ɹ']\n",
      "[3, 6, 14, 22, 24, 25, 31, 34, 35, 39, 41, 42, 48, 52, 56, 61, 63]\n",
      "[3, 6, 14, -1, 22, 24, 25, 31, 34, 35, 39, 41, 42, 48, 52, 56, 61, 63]\n",
      "\n",
      "\n",
      "the kitchen window was clean\n",
      "['DH', 'AH0', 'K', 'IH1', 'CH', 'AH0', 'N', 'W', 'IH1', 'N', 'D', 'OW0', 'W', 'AH0', 'Z', 'K', 'L', 'IY1', 'N']\n",
      "['DH', 'AH0', 'K', 'IH1', 'CH', 'AH0', 'N', 'W', 'IH1', 'N', 'D', 'OW0', 'W', 'AA1', 'Z', 'K', 'L', 'IY1', 'N']\n",
      "['ð', 'ʌ', 'k', 'ɪ', 'tʃ', 'ʌ', 'n', 'w', 'ɪ', 'n', 'd', 'oʊ', 'w', 'ɑ', 'z', 'k', 'l', 'i', 'n']\n",
      "['ð', 'ʌ', 'k', 'ɪ', 'tʃ', 'ɪ', 'n', 'w', 'ɪ', 'n', 'd', 'oʊ', 'w', 'ʌ', 'z', 'k', 'l', 'i', 'n']\n",
      "[2, 3, 7, 10, 14, 16, 19, 23, 25, 29, 31, 33, 40, 41, 45, 49, 51, 54, 65]\n",
      "[2, 3, 7, 10, 14, 16, 19, 23, 25, 29, 31, 33, 40, 41, 45, 49, 51, 54, 65]\n",
      "\n",
      "\n",
      "he hung up his raincoat\n",
      "['HH', 'IY1', 'HH', 'AH1', 'NG', 'AH1', 'P', 'HH', 'AH0', 'Z', 'R', 'EY1', 'N', 'K', 'OW2', 'T']\n",
      "['HH', 'IY1', 'HH', 'AH1', 'NG', 'AH1', 'P', 'HH', 'IH1', 'Z', 'R', 'EY1', 'N', 'K', 'OW2', 'T']\n",
      "['h', 'i', 'h', 'ʌ', 'ŋ', 'ʌ', 'p', 'h', 'ɪ', 'z', 'ɹ', 'eɪ', 'n', 'k', 'oʊ', 't']\n",
      "['h', 'i', 'h', 'ʌ', 'ŋ', 'ʌ', 'p', 'h', 'ɪ', 'z', 'ɹ', 'eɪ', 'n', 'k', 'oʊ', 't']\n",
      "[1, 2, 8, 11, 16, 18, 24, 26, 27, 31, 35, 38, 44, 48, 52, 60]\n",
      "[1, 2, 8, 11, 16, 18, 24, 26, 27, 31, 35, 38, 44, 48, 52, 60]\n",
      "\n",
      "\n",
      "the mailman brought a letter\n",
      "['DH', 'AH0', 'M', 'EY1', 'L', 'M', 'AE2', 'N', 'B', 'R', 'AO1', 'T', 'AH0', 'L', 'EH1', 'T', 'ER0']\n",
      "['DH', 'AH0', 'M', 'EY1', 'L', 'M', 'AE2', 'N', 'B', 'R', 'AO1', 'T', 'AH0', 'L', 'EH1', 'T', 'ER0']\n",
      "['ð', 'ʌ', 'm', 'eɪ', 'l', 'm', 'æ', 'n', 'b', 'ɹ', 'ɒ', 't', 'ʌ', 'l', 'e', 't', 'ɜ']\n",
      "['ð', 'm', 'eɪ', 'l', 'm', 'æ', 'n', 'b', 'ɹ', 'aʊ', 't', 'ʌ', 'l', 'æ', 'ɹ']\n",
      "[2, 8, 10, 15, 19, 21, 26, 30, 31, 33, 37, 39, 44, 46, 53]\n",
      "[2, -1, 8, 10, 15, 19, 21, 26, 30, 31, 33, 37, 39, 44, 46, 53, -1]\n",
      "\n",
      "\n",
      "the mother heard the baby\n",
      "['DH', 'AH0', 'M', 'AH1', 'DH', 'ER0', 'HH', 'ER1', 'D', 'DH', 'AH0', 'B', 'EY1', 'B', 'IY0']\n",
      "['DH', 'AH0', 'M', 'AH1', 'DH', 'ER0', 'HH', 'ER1', 'D', 'DH', 'AH0', 'B', 'EY1', 'B', 'IY0']\n",
      "['ð', 'ʌ', 'm', 'ʌ', 'ð', 'ɜ', 'h', 'ɜ', 'd', 'ð', 'ʌ', 'b', 'eɪ', 'b', 'i']\n",
      "['ð', 'ʌ', 'm', 'ʌ', 'ð', 'ɹ', 'h', 'ɹ', 'd', 'ð', 'ʌ', 'b', 'eɪ', 'b', 'i']\n",
      "[1, 2, 7, 9, 14, 16, 22, 25, 30, 33, 34, 38, 40, 48, 50]\n",
      "[1, 2, 7, 9, 14, 16, 22, 25, 30, 33, 34, 38, 40, 48, 50]\n",
      "\n",
      "\n",
      "she found her purse in the trash\n",
      "['SH', 'IY1', 'F', 'AW1', 'N', 'D', 'HH', 'ER1', 'P', 'ER1', 'S', 'AH0', 'N', 'DH', 'AH0', 'T', 'R', 'AE1', 'SH']\n",
      "['SH', 'IY1', 'F', 'AW1', 'N', 'D', 'HH', 'ER0', 'P', 'ER1', 'S', 'IH0', 'N', 'DH', 'AH0', 'T', 'R', 'AE1', 'SH']\n",
      "['ʃ', 'i', 'f', 'aʊ', 'n', 'd', 'h', 'ɜ', 'p', 'ɜ', 's', 'ɪ', 'n', 'ð', 'ʌ', 't', 'ɹ', 'æ', 'ʃ']\n",
      "['ʃ', 'i', 'f', 'aʊ', 'n', 'd', 'h', 'ɹ', 'p', 'ɹ', 's', 'ɪ', 'n', 'ð', 'ɪ', 't', 'ɹ', 'æ', 'ʃ']\n",
      "[2, 5, 12, 15, 22, 23, 25, 27, 33, 37, 45, 51, 54, 56, 57, 62, 65, 68, 81]\n",
      "[2, 5, 12, 15, 22, 23, 25, 27, 33, 37, 45, 51, 54, 56, 57, 62, 65, 68, 81]\n",
      "\n",
      "\n",
      "the table has three legs\n",
      "['DH', 'AH0', 'T', 'EY1', 'B', 'AH0', 'L', 'HH', 'AE1', 'Z', 'TH', 'R', 'IY1', 'L', 'EH1', 'G', 'Z']\n",
      "['DH', 'AH0', 'T', 'EY1', 'B', 'AH0', 'L', 'HH', 'AE1', 'Z', 'TH', 'R', 'IY1', 'L', 'EH1', 'G', 'Z']\n",
      "['ð', 'ʌ', 't', 'eɪ', 'b', 'ʌ', 'l', 'h', 'æ', 'z', 'θ', 'ɹ', 'i', 'l', 'e', 'ɡ', 'z']\n",
      "['ð', 'ɪ', 't', 'eɪ', 'b', 'oʊ', 'h', 'æ', 'z', 'θ', 'ɹ', 'i', 'l', 'æ', 'ɡ', 'z']\n",
      "[1, 3, 7, 11, 18, 20, 26, 29, 35, 41, 44, 46, 52, 55, 64, 67]\n",
      "[1, 3, 7, 11, 18, 20, -1, 26, 29, 35, 41, 44, 46, 52, 55, 64, 67]\n",
      "\n",
      "\n",
      "the children waved at the train\n",
      "['DH', 'AH0', 'CH', 'IH1', 'L', 'D', 'R', 'AH0', 'N', 'W', 'EY1', 'V', 'D', 'AH0', 'T', 'DH', 'AH0', 'T', 'R', 'EY1', 'N']\n",
      "['DH', 'AH0', 'CH', 'IH1', 'L', 'D', 'R', 'AH0', 'N', 'W', 'EY1', 'V', 'D', 'AE1', 'T', 'DH', 'AH0', 'T', 'R', 'EY1', 'N']\n",
      "['ð', 'ʌ', 'tʃ', 'ɪ', 'l', 'd', 'ɹ', 'ʌ', 'n', 'w', 'eɪ', 'v', 'd', 'æ', 't', 'ð', 'ʌ', 't', 'ɹ', 'eɪ', 'n']\n",
      "['ð', 'ʌ', 'tʃ', 'ɪ', 'l', 'd', 'ɹ', 'ʌ', 'n', 'w', 'eɪ', 'v', 'd', 'æ', 'ð', 't', 'ɹ', 'eɪ', 'n']\n",
      "[2, 3, 8, 12, 15, 18, 20, 21, 25, 28, 31, 38, 41, 43, 48, 53, 56, 59, 69]\n",
      "[2, 3, 8, 12, 15, 18, 20, 21, 25, 28, 31, 38, 41, 43, -1, 48, -1, 53, 56, 59, 69]\n",
      "\n",
      "\n",
      "her coat is on a chair\n",
      "['HH', 'ER0', 'K', 'OW1', 'T', 'IH1', 'Z', 'AA1', 'N', 'AH0', 'CH', 'EH1', 'R']\n",
      "['HH', 'ER0', 'K', 'OW1', 'T', 'IH1', 'Z', 'AA1', 'N', 'AH0', 'CH', 'EH1', 'R']\n",
      "['h', 'ɜ', 'k', 'oʊ', 't', 'ɪ', 'z', 'ɑ', 'n', 'ʌ', 'tʃ', 'e', 'ɹ']\n",
      "['h', 'ɹ', 'k', 'oʊ', 't', 'ɪ', 'z', 'aʊ', 'n', 'ʌ', 'tʃ', 'eɪ']\n",
      "[2, 4, 10, 14, 21, 25, 28, 34, 38, 40, 45, 50]\n",
      "[2, 4, 10, 14, 21, 25, 28, 34, 38, 40, 45, 50, -1]\n",
      "\n",
      "\n",
      "the girl is fixing her dress\n",
      "['DH', 'IY0', 'G', 'ER1', 'L', 'AH0', 'Z', 'F', 'IH1', 'K', 'S', 'IH0', 'NG', 'HH', 'ER0', 'D', 'R', 'EH1', 'S']\n",
      "['DH', 'AH0', 'G', 'ER1', 'L', 'IH1', 'Z', 'F', 'IH1', 'K', 'S', 'IH0', 'NG', 'HH', 'ER0', 'D', 'R', 'EH1', 'S']\n",
      "['ð', 'ʌ', 'ɡ', 'ɜ', 'l', 'ɪ', 'z', 'f', 'ɪ', 'k', 's', 'ɪ', 'ŋ', 'h', 'ɜ', 'd', 'ɹ', 'e', 's']\n",
      "['ð', 'ʌ', 'ɡ', 'ɹ', 'l', 'ɪ', 'z', 'f', 'ɪ', 'k', 's', 'ɪ', 'ŋ', 'h', 'ɹ', 'd', 'ɹ', 'æ', 's']\n",
      "[2, 3, 8, 11, 19, 23, 26, 31, 33, 36, 38, 42, 45, 48, 50, 55, 57, 59, 68]\n",
      "[2, 3, 8, 11, 19, 23, 26, 31, 33, 36, 38, 42, 45, 48, 50, 55, 57, 59, 68]\n",
      "\n",
      "\n",
      "it's time to go to bed\n",
      "['IH1', 'T', 'S', 'T', 'AY1', 'M', 'T', 'IH0', 'G', 'OW1', 'T', 'AH0', 'B', 'EH1', 'D']\n",
      "['IH1', 'T', 'S', 'T', 'AY1', 'M', 'T', 'UW1', 'G', 'OW1', 'T', 'UW1', 'B', 'EH1', 'D']\n",
      "['ɪ', 't', 's', 't', 'aɪ', 'm', 't', 'u', 'ɡ', 'oʊ', 't', 'u', 'b', 'e', 'd']\n",
      "['ɪ', 't', 's', 't', 'aɪ', 'm', 't', 'ɡ', 'oʊ', 't', 'ʌ', 'b', 'æ', 'd']\n",
      "[2, 6, 7, 11, 15, 21, 25, 30, 32, 38, 39, 43, 46, 55]\n",
      "[2, 6, 7, 11, 15, 21, 25, -1, 30, 32, 38, 39, 43, 46, 55]\n",
      "\n",
      "\n",
      "mother read the instructions\n",
      "['M', 'AH1', 'DH', 'ER0', 'R', 'EH1', 'D', 'DH', 'IY0', 'IH2', 'N', 'S', 'T', 'R', 'AH1', 'K', 'SH', 'AH0', 'N', 'Z']\n",
      "['M', 'AH1', 'DH', 'ER0', 'R', 'EH1', 'D', 'DH', 'AH0', 'IH0', 'N', 'S', 'T', 'R', 'AH1', 'K', 'SH', 'AH0', 'N', 'Z']\n",
      "['m', 'ʌ', 'ð', 'ɜ', 'ɹ', 'e', 'd', 'ð', 'ʌ', 'ɪ', 'n', 's', 't', 'ɹ', 'ʌ', 'k', 'ʃ', 'ʌ', 'n', 'z']\n",
      "['m', 'ʌ', 'ð', 'oʊ', 'ɹ', 'æ', 'd', 'ð', 'ɪ', 'ɪ', 'n', 's', 't', 'ɹ', 'ʌ', 'k', 'ʃ', 'ɪ', 'n', 'z']\n",
      "[3, 5, 10, 12, 18, 20, 26, 28, 30, 33, 36, 39, 42, 44, 45, 50, 53, 57, 62, 68]\n",
      "[3, 5, 10, 12, 18, 20, 26, 28, 30, 33, 36, 39, 42, 44, 45, 50, 53, 57, 62, 68]\n",
      "\n",
      "\n",
      "the dog is eating some meat\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'IH1', 'Z', 'IY1', 'T', 'IH0', 'NG', 'S', 'AH1', 'M', 'M', 'IY1', 'T']\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'IH1', 'Z', 'IY1', 'T', 'IH0', 'NG', 'S', 'AH1', 'M', 'M', 'IY1', 'T']\n",
      "['ð', 'ʌ', 'd', 'ɒ', 'ɡ', 'ɪ', 'z', 'i', 't', 'ɪ', 'ŋ', 's', 'ʌ', 'm', 'm', 'i', 't']\n",
      "['ð', 'd', 'aʊ', 'ɡ', 'ɪ', 'z', 'ɪ', 'd', 'ɪ', 'ŋ', 's', 'ʌ', 'm', 'm', 'ɪ', 't']\n",
      "[2, 8, 12, 21, 25, 28, 33, 37, 39, 41, 46, 49, 53, 56, 58, 67]\n",
      "[2, -1, 8, 12, 21, 25, 28, 33, 37, 39, 41, 46, 49, 53, 56, 58, 67]\n",
      "\n",
      "\n",
      "father forgot the bread\n",
      "['F', 'AA1', 'DH', 'ER0', 'F', 'ER0', 'G', 'AA1', 'T', 'DH', 'AH0', 'B', 'R', 'EH1', 'D']\n",
      "['F', 'AA1', 'DH', 'ER0', 'F', 'ER0', 'G', 'AA1', 'T', 'DH', 'AH0', 'B', 'R', 'EH1', 'D']\n",
      "['f', 'ɑ', 'ð', 'ɜ', 'f', 'ɜ', 'ɡ', 'ɑ', 't', 'ð', 'ʌ', 'b', 'ɹ', 'e', 'd']\n",
      "['f', 'æ', 'ð', 'ɪ', 'f', 'ɹ', 'ɡ', 'æ', 't', 'ð', 'ɪ', 'b', 'ɹ', 'æ', 'd']\n",
      "[2, 5, 11, 13, 19, 21, 26, 28, 34, 37, 38, 42, 44, 47, 55]\n",
      "[2, 5, 11, 13, 19, 21, 26, 28, 34, 37, 38, 42, 44, 47, 55]\n",
      "\n",
      "\n",
      "the road goes up a hill\n",
      "['DH', 'AH0', 'R', 'OW1', 'D', 'G', 'OW1', 'Z', 'AH1', 'P', 'AH0', 'HH', 'IH1', 'L']\n",
      "['DH', 'AH0', 'R', 'OW1', 'D', 'G', 'OW1', 'Z', 'AH1', 'P', 'AH0', 'HH', 'IH1', 'L']\n",
      "['ð', 'ʌ', 'ɹ', 'oʊ', 'd', 'ɡ', 'oʊ', 'z', 'ʌ', 'p', 'ʌ', 'h', 'ɪ', 'l']\n",
      "['ð', 'ɹ', 'oʊ', 'd', 'ɡ', 'oʊ', 'z', 'ʌ', 'p', 'ʌ', 'h', 'ɪ', 'l']\n",
      "[2, 9, 12, 19, 23, 26, 34, 39, 45, 47, 51, 55, 61]\n",
      "[2, -1, 9, 12, 19, 23, 26, 34, 39, 45, 47, 51, 55, 61]\n",
      "\n",
      "\n",
      "the painter uses a brush\n",
      "['DH', 'AH0', 'P', 'EY1', 'N', 'T', 'ER0', 'Y', 'UW1', 'Z', 'AH0', 'Z', 'AH0', 'B', 'R', 'AH1', 'SH']\n",
      "['DH', 'AH0', 'P', 'EY1', 'N', 'T', 'ER0', 'Y', 'UW1', 'S', 'AH0', 'Z', 'AH0', 'B', 'R', 'AH1', 'SH']\n",
      "['ð', 'ʌ', 'p', 'eɪ', 'n', 't', 'ɜ', 'j', 'u', 's', 'ʌ', 'z', 'ʌ', 'b', 'ɹ', 'ʌ', 'ʃ']\n",
      "['ð', 'ʌ', 'p', 'eɪ', 'n', 't', 'j', 'u', 'z', 'ɪ', 'z', 'ʌ', 'b', 'ɹ', 'ʌ', 'ʃ']\n",
      "[2, 3, 7, 11, 15, 18, 27, 30, 36, 38, 42, 44, 49, 50, 53, 63]\n",
      "[2, 3, 7, 11, 15, 18, -1, 27, 30, 36, 38, 42, 44, 49, 50, 53, 63]\n",
      "\n",
      "\n",
      "the family bought a house\n",
      "['DH', 'AH0', 'F', 'AE1', 'M', 'L', 'IY0', 'B', 'AA1', 'T', 'AH0', 'HH', 'AW1', 'S']\n",
      "['DH', 'AH0', 'F', 'AE1', 'M', 'AH0', 'L', 'IY0', 'B', 'AA1', 'T', 'AH0', 'HH', 'AW1', 'S']\n",
      "['ð', 'ʌ', 'f', 'æ', 'm', 'ʌ', 'l', 'i', 'b', 'ɑ', 't', 'ʌ', 'h', 'aʊ', 's']\n",
      "['ð', 'ʌ', 'f', 'æ', 'm', 'l', 'i', 'b', 'aʊ', 't', 'ʌ', 'h', 'aʊ', 's']\n",
      "[2, 3, 9, 12, 18, 20, 22, 28, 31, 39, 43, 48, 51, 64]\n",
      "[2, 3, 9, 12, 18, -1, 20, 22, 28, 31, 39, 43, 48, 51, 64]\n",
      "\n",
      "\n",
      "swimmers can hold their breath\n",
      "['S', 'W', 'IH1', 'M', 'ER0', 'Z', 'K', 'AH0', 'N', 'HH', 'OW1', 'L', 'D', 'DH', 'EH1', 'R', 'B', 'R', 'EH1', 'TH']\n",
      "['S', 'W', 'IH1', 'M', 'ER0', 'Z', 'K', 'AE1', 'N', 'HH', 'OW1', 'L', 'D', 'DH', 'EH1', 'R', 'B', 'R', 'EH1', 'TH']\n",
      "['s', 'w', 'ɪ', 'm', 'ɜ', 'z', 'k', 'æ', 'n', 'h', 'oʊ', 'l', 'd', 'ð', 'e', 'ɹ', 'b', 'ɹ', 'e', 'θ']\n",
      "['s', 'w', 'ɪ', 'm', 'ɪ', 'z', 'k', 'æ', 'n', 'h', 'oʊ', 'l', 'd', 'ð', 'æ', 'b', 'ɹ', 'æ', 'θ']\n",
      "[4, 8, 10, 14, 16, 22, 26, 28, 30, 33, 36, 40, 42, 45, 47, 53, 54, 57, 68]\n",
      "[4, 8, 10, 14, 16, 22, 26, 28, 30, 33, 36, 40, 42, 45, 47, -1, 53, 54, 57, 68]\n",
      "\n",
      "\n",
      "she cut the steak with her knife\n",
      "['SH', 'IY1', 'K', 'AH1', 'T', 'DH', 'AH0', 'S', 'T', 'EY1', 'K', 'W', 'IH1', 'TH', 'HH', 'ER1', 'N', 'AY1', 'F']\n",
      "['SH', 'IY1', 'K', 'AH1', 'T', 'DH', 'AH0', 'S', 'T', 'EY1', 'K', 'W', 'IH1', 'DH', 'HH', 'ER0', 'N', 'AY1', 'F']\n",
      "['ʃ', 'i', 'k', 'ʌ', 't', 'ð', 'ʌ', 's', 't', 'eɪ', 'k', 'w', 'ɪ', 'ð', 'h', 'ɜ', 'n', 'aɪ', 'f']\n",
      "['ʃ', 'i', 'k', 'ʌ', 't', 'ð', 'ʌ', 's', 't', 'eɪ', 'k', 'w', 'ɪ', 'ð', 'h', 'ɹ', 'n', 'aɪ', 'f']\n",
      "[2, 5, 10, 14, 19, 22, 23, 27, 31, 34, 41, 44, 45, 49, 50, 53, 58, 61, 72]\n",
      "[2, 5, 10, 14, 19, 22, 23, 27, 31, 34, 41, 44, 45, 49, 50, 53, 58, 61, 72]\n",
      "\n",
      "\n",
      "they're pushing an old car\n",
      "['DH', 'EH1', 'R', 'P', 'UH1', 'SH', 'IH0', 'NG', 'AE1', 'N', 'OW1', 'L', 'D', 'K', 'AA1', 'R']\n",
      "['DH', 'EH1', 'R', 'P', 'UH1', 'SH', 'IH0', 'NG', 'AE1', 'N', 'OW1', 'L', 'D', 'K', 'AA1', 'R']\n",
      "['ð', 'e', 'ɹ', 'p', 'ʊ', 'ʃ', 'ɪ', 'ŋ', 'æ', 'n', 'oʊ', 'l', 'd', 'k', 'ɑ', 'ɹ']\n",
      "['ð', 'æ', 'p', 'ʊ', 'ʃ', 'ɪ', 'ŋ', 'æ', 'n', 'oʊ', 'l', 'd', 'k', 'aʊ']\n",
      "[2, 4, 9, 12, 17, 21, 25, 29, 32, 35, 39, 41, 46, 50]\n",
      "[2, 4, -1, 9, 12, 17, 21, 25, 29, 32, 35, 39, 41, 46, 50, -1]\n",
      "\n",
      "\n",
      "the food is expensive\n",
      "['DH', 'AH0', 'F', 'UW1', 'D', 'IH1', 'Z', 'IH0', 'K', 'S', 'P', 'EH1', 'N', 'S', 'IH0', 'V']\n",
      "['DH', 'AH0', 'F', 'UW1', 'D', 'IH1', 'Z', 'IH0', 'K', 'S', 'P', 'EH1', 'N', 'S', 'IH0', 'V']\n",
      "['ð', 'ʌ', 'f', 'u', 'd', 'ɪ', 'z', 'ɪ', 'k', 's', 'p', 'e', 'n', 's', 'ɪ', 'v']\n",
      "['ð', 'ɪ', 'f', 'ʊ', 'd', 'ɪ', 'z', 'ɪ', 'k', 's', 'p', 'æ', 'n', 's', 'ɪ', 'v']\n",
      "[2, 3, 9, 12, 21, 24, 28, 30, 33, 35, 39, 41, 45, 48, 52, 60]\n",
      "[2, 3, 9, 12, 21, 24, 28, 30, 33, 35, 39, 41, 45, 48, 52, 60]\n",
      "\n",
      "\n",
      "the children are walking home\n",
      "['DH', 'AH0', 'CH', 'IH1', 'L', 'D', 'R', 'AH0', 'N', 'AA1', 'R', 'W', 'AO1', 'K', 'IH0', 'NG', 'HH', 'OW1', 'M']\n",
      "['DH', 'AH0', 'CH', 'IH1', 'L', 'D', 'R', 'AH0', 'N', 'AA1', 'R', 'W', 'AO1', 'K', 'IH0', 'NG', 'HH', 'OW1', 'M']\n",
      "['ð', 'ʌ', 'tʃ', 'ɪ', 'l', 'd', 'ɹ', 'ʌ', 'n', 'ɑ', 'ɹ', 'w', 'ɒ', 'k', 'ɪ', 'ŋ', 'h', 'oʊ', 'm']\n",
      "['ð', 'ʌ', 'tʃ', 'ɪ', 'l', 'd', 'ɹ', 'ɪ', 'n', 'aʊ', 'w', 'aʊ', 'k', 'ɪ', 'ŋ', 'h', 'oʊ', 'm']\n",
      "[2, 3, 8, 12, 15, 19, 20, 22, 26, 30, 36, 38, 45, 48, 51, 54, 58, 67]\n",
      "[2, 3, 8, 12, 15, 19, 20, 22, 26, 30, -1, 36, 38, 45, 48, 51, 54, 58, 67]\n",
      "\n",
      "\n",
      "they had two empty bottles\n",
      "['DH', 'EY1', 'HH', 'AE1', 'D', 'T', 'UW1', 'EH1', 'M', 'T', 'IY0', 'B', 'AA1', 'T', 'AH0', 'L', 'Z']\n",
      "['DH', 'EY1', 'HH', 'AE1', 'D', 'T', 'UW1', 'EH1', 'M', 'P', 'T', 'IY0', 'B', 'AA1', 'T', 'AH0', 'L', 'Z']\n",
      "['ð', 'eɪ', 'h', 'æ', 'd', 't', 'u', 'e', 'm', 'p', 't', 'i', 'b', 'ɑ', 't', 'ʌ', 'l', 'z']\n",
      "['ð', 'eɪ', 'h', 'æ', 'd', 't', 'u', 'æ', 'm', 'p', 't', 'i', 'b', 'aʊ', 't', 'oʊ', 'z']\n",
      "[2, 3, 8, 10, 14, 18, 22, 29, 32, 33, 36, 38, 43, 45, 52, 54, 63]\n",
      "[2, 3, 8, 10, 14, 18, 22, 29, 32, 33, 36, 38, 43, 45, 52, 54, -1, 63]\n",
      "\n",
      "\n",
      "milk comes in a carton\n",
      "['M', 'IH1', 'L', 'K', 'K', 'AH1', 'M', 'Z', 'AH0', 'N', 'AH0', 'K', 'AA1', 'R', 'T', 'AH0', 'N']\n",
      "['M', 'IH1', 'L', 'K', 'K', 'AH1', 'M', 'Z', 'IH0', 'N', 'AH0', 'K', 'AA1', 'R', 'T', 'AH0', 'N']\n",
      "['m', 'ɪ', 'l', 'k', 'k', 'ʌ', 'm', 'z', 'ɪ', 'n', 'ʌ', 'k', 'ɑ', 'ɹ', 't', 'ʌ', 'n']\n",
      "['m', 'ɪ', 'l', 'k', 'k', 'ʌ', 'm', 'z', 'ɪ', 'n', 'ʌ', 'k', 'aɪ', 't', 'ɪ', 'n']\n",
      "[3, 5, 7, 11, 15, 19, 25, 29, 32, 34, 36, 41, 45, 52, 55, 58]\n",
      "[3, 5, 7, 11, 15, 19, 25, 29, 32, 34, 36, 41, 45, -1, 52, 55, 58]\n",
      "\n",
      "\n",
      "the dog sleeps in a basket\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'S', 'L', 'IY1', 'P', 'S', 'AH0', 'N', 'AH0', 'B', 'AE1', 'S', 'K', 'AH0', 'T']\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'S', 'L', 'IY1', 'P', 'S', 'IH0', 'N', 'AH0', 'B', 'AE1', 'S', 'K', 'AH0', 'T']\n",
      "['ð', 'ʌ', 'd', 'ɒ', 'ɡ', 's', 'l', 'i', 'p', 's', 'ɪ', 'n', 'ʌ', 'b', 'æ', 's', 'k', 'ʌ', 't']\n",
      "['ð', 'd', 'aʊ', 'ɡ', 's', 'l', 'i', 'p', 's', 'ɪ', 'n', 'ʌ', 'b', 'æ', 's', 'k', 'ɪ', 't']\n",
      "[2, 8, 11, 19, 23, 28, 30, 34, 36, 39, 42, 43, 47, 50, 58, 62, 65, 71]\n",
      "[2, -1, 8, 11, 19, 23, 28, 30, 34, 36, 39, 42, 43, 47, 50, 58, 62, 65, 71]\n",
      "\n",
      "\n",
      "the house had nine bedrooms\n",
      "['DH', 'AH1', 'HH', 'AW1', 'S', 'HH', 'AE1', 'D', 'N', 'AY1', 'N', 'B', 'EH1', 'D', 'R', 'UW2', 'M', 'Z']\n",
      "['DH', 'AH0', 'HH', 'AW1', 'S', 'HH', 'AE1', 'D', 'N', 'AY1', 'N', 'B', 'EH1', 'D', 'R', 'UW2', 'M', 'Z']\n",
      "['ð', 'ʌ', 'h', 'aʊ', 's', 'h', 'æ', 'd', 'n', 'aɪ', 'n', 'b', 'e', 'd', 'ɹ', 'u', 'm', 'z']\n",
      "['ð', 'ʌ', 'h', 'aʊ', 's', 'h', 'æ', 'd', 'n', 'aɪ', 'n', 'b', 'e', 'd', 'ɹ', 'ʊ', 'm', 'z']\n",
      "[2, 3, 7, 10, 19, 22, 25, 30, 35, 37, 45, 49, 51, 56, 58, 60, 67, 71]\n",
      "[2, 3, 7, 10, 19, 22, 25, 30, 35, 37, 45, 49, 51, 56, 58, 60, 67, 71]\n",
      "\n",
      "\n",
      "they're shopping for school clothes\n",
      "['DH', 'EH1', 'R', 'SH', 'AA1', 'P', 'AH0', 'N', 'F', 'R', 'ER0', 'S', 'K', 'UW1', 'L', 'K', 'L', 'OW1', 'Z']\n",
      "['DH', 'EH1', 'R', 'SH', 'AA1', 'P', 'IH0', 'NG', 'F', 'AO1', 'R', 'S', 'K', 'UW1', 'L', 'K', 'L', 'OW1', 'DH', 'Z']\n",
      "['ð', 'e', 'ɹ', 'ʃ', 'ɑ', 'p', 'ɪ', 'ŋ', 'f', 'ɒ', 'ɹ', 's', 'k', 'u', 'l', 'k', 'l', 'oʊ', 'ð', 'z']\n",
      "['ð', 'æ', 'ʃ', 'aʊ', 'p', 'ɪ', 'ŋ', 'f', 'ɹ', 's', 'k', 'w', 'l', 'k', 'l', 'oʊ', 'ð', 'z']\n",
      "[1, 3, 9, 14, 20, 22, 25, 29, 31, 35, 40, 42, 46, 51, 54, 56, 67, 71]\n",
      "[1, 3, -1, 9, 14, 20, 22, 25, 29, -1, 31, 35, 40, 42, 46, 51, 54, 56, 67, 71]\n",
      "\n",
      "\n",
      "they're playing in the park\n",
      "['DH', 'EH1', 'R', 'P', 'L', 'EY1', 'IH0', 'NG', 'IH1', 'N', 'DH', 'AH1', 'P', 'AA1', 'R', 'K']\n",
      "['DH', 'EH1', 'R', 'P', 'L', 'EY1', 'IH0', 'NG', 'IH0', 'N', 'DH', 'AH0', 'P', 'AA1', 'R', 'K']\n",
      "['ð', 'e', 'ɹ', 'p', 'l', 'eɪ', 'ɪ', 'ŋ', 'ɪ', 'n', 'ð', 'ʌ', 'p', 'ɑ', 'ɹ', 'k']\n",
      "['ð', 'æ', 'p', 'l', 'eɪ', 'ɪ', 'ŋ', 'ɪ', 'n', 'ð', 'p', 'æ', 'k']\n",
      "[1, 3, 8, 11, 13, 19, 22, 24, 26, 28, 33, 37, 50]\n",
      "[1, 3, -1, 8, 11, 13, 19, 22, 24, 26, 28, -1, 33, 37, -1, 50]\n",
      "\n",
      "\n",
      "rain is good for trees\n",
      "['R', 'EY1', 'N', 'AH0', 'Z', 'G', 'AH0', 'D', 'F', 'R', 'ER0', 'T', 'R', 'IY1', 'Z']\n",
      "['R', 'EY1', 'N', 'IH1', 'Z', 'G', 'UH1', 'D', 'F', 'AO1', 'R', 'T', 'R', 'IY1', 'Z']\n",
      "['ɹ', 'eɪ', 'n', 'ɪ', 'z', 'ɡ', 'ʊ', 'd', 'f', 'ɒ', 'ɹ', 't', 'ɹ', 'i', 'z']\n",
      "['ɹ', 'eɪ', 'n', 'ɪ', 'z', 'ɡ', 'ʊ', 'd', 'f', 'ɹ', 't', 'ɹ', 'i', 'z']\n",
      "[4, 7, 15, 20, 23, 28, 29, 33, 36, 39, 44, 48, 51, 64]\n",
      "[4, 7, 15, 20, 23, 28, 29, 33, 36, -1, 39, 44, 48, 51, 64]\n",
      "\n",
      "\n",
      "they sat on a wooden bench\n",
      "['DH', 'EY1', 'S', 'AE1', 'T', 'AA1', 'N', 'AH0', 'W', 'UH1', 'D', 'AH0', 'N', 'B', 'EH1', 'N', 'CH']\n",
      "['DH', 'EY1', 'S', 'AE1', 'T', 'AA1', 'N', 'AH0', 'W', 'UH1', 'D', 'AH0', 'N', 'B', 'EH1', 'N', 'CH']\n",
      "['ð', 'eɪ', 's', 'æ', 't', 'ɑ', 'n', 'ʌ', 'w', 'ʊ', 'd', 'ʌ', 'n', 'b', 'e', 'n', 'tʃ']\n",
      "['ð', 'eɪ', 's', 'æ', 't', 'aʊ', 'n', 'ʌ', 'w', 'ʊ', 'd', 'ɪ', 'n', 'b', 'e', 'n', 'tʃ']\n",
      "[1, 2, 8, 14, 21, 24, 27, 29, 34, 36, 40, 42, 44, 50, 52, 58, 64]\n",
      "[1, 2, 8, 14, 21, 24, 27, 29, 34, 36, 40, 42, 44, 50, 52, 58, 64]\n",
      "\n",
      "\n",
      "the child drank some fresh milk\n",
      "['DH', 'AH0', 'CH', 'AY1', 'L', 'D', 'D', 'R', 'AE1', 'NG', 'K', 'S', 'AH1', 'M', 'F', 'R', 'EH1', 'SH', 'M', 'IH1', 'L', 'K']\n",
      "['DH', 'AH0', 'CH', 'AY1', 'L', 'D', 'D', 'R', 'AE1', 'NG', 'K', 'S', 'AH1', 'M', 'F', 'R', 'EH1', 'SH', 'M', 'IH1', 'L', 'K']\n",
      "['ð', 'ʌ', 'tʃ', 'aɪ', 'l', 'd', 'd', 'ɹ', 'æ', 'ŋ', 'k', 's', 'ʌ', 'm', 'f', 'ɹ', 'e', 'ʃ', 'm', 'ɪ', 'l', 'k']\n",
      "['ð', 'ɪ', 'tʃ', 'aɪ', 'l', 'd', 'd', 'ɹ', 'æ', 'ŋ', 'k', 's', 'ʌ', 'm', 'f', 'ɹ', 'æ', 'ʃ', 'm', 'ɪ', 'l', 'k']\n",
      "[1, 3, 8, 13, 21, 24, 28, 30, 32, 37, 39, 43, 47, 50, 54, 56, 58, 64, 69, 71, 76, 82]\n",
      "[1, 3, 8, 13, 21, 24, 28, 30, 32, 37, 39, 43, 47, 50, 54, 56, 58, 64, 69, 71, 76, 82]\n",
      "\n",
      "\n",
      "the baby slept all night\n",
      "['DH', 'AH0', 'B', 'EY1', 'B', 'IY0', 'S', 'L', 'EH1', 'P', 'T', 'AO1', 'L', 'N', 'AY1', 'T']\n",
      "['DH', 'AH0', 'B', 'EY1', 'B', 'IY0', 'S', 'L', 'EH1', 'P', 'T', 'AO1', 'L', 'N', 'AY1', 'T']\n",
      "['ð', 'ʌ', 'b', 'eɪ', 'b', 'i', 's', 'l', 'e', 'p', 't', 'ɒ', 'l', 'n', 'aɪ', 't']\n",
      "['ð', 'b', 'eɪ', 'b', 'i', 's', 'l', 'e', 'p', 't', 'aʊ', 'l', 'n', 'aɪ', 't']\n",
      "[2, 7, 9, 15, 17, 22, 27, 29, 34, 37, 41, 45, 49, 52, 62]\n",
      "[2, -1, 7, 9, 15, 17, 22, 27, 29, 34, 37, 41, 45, 49, 52, 62]\n",
      "\n",
      "\n",
      "the salt shaker is empty\n",
      "['DH', 'AH0', 'S', 'AO1', 'L', 'T', 'SH', 'EY1', 'K', 'ER0', 'IH1', 'Z', 'EH1', 'M', 'T', 'IY0']\n",
      "['DH', 'AH0', 'S', 'AO1', 'L', 'T', 'SH', 'EY1', 'K', 'ER0', 'IH1', 'Z', 'EH1', 'M', 'P', 'T', 'IY0']\n",
      "['ð', 'ʌ', 's', 'ɒ', 'l', 't', 'ʃ', 'eɪ', 'k', 'ɜ', 'ɪ', 'z', 'e', 'm', 'p', 't', 'i']\n",
      "['ð', 'ʌ', 's', 'aʊ', 'l', 't', 'ʃ', 'eɪ', 'k', 'ɹ', 'ɹ', 'ɪ', 'z', 'æ', 'm', 'p', 't', 'i']\n",
      "[1, 2, 7, 12, 15, 18, 22, 25, 31, 34, 36, 39, 42, 46, 49, 51, 55, 57]\n",
      "[1, 2, 7, 12, 15, 18, 22, 25, 31, 34, -1, 39, 42, 46, 49, 51, 55, 57]\n",
      "\n",
      "\n",
      "the policeman knows the way\n",
      "['DH', 'AH0', 'P', 'AH0', 'L', 'IY1', 'S', 'M', 'AH0', 'N', 'N', 'OW1', 'Z', 'DH', 'AH0', 'W', 'EY1']\n",
      "['DH', 'AH0', 'P', 'AH0', 'L', 'IY1', 'S', 'M', 'AH0', 'N', 'N', 'OW1', 'Z', 'DH', 'AH0', 'W', 'EY1']\n",
      "['ð', 'ʌ', 'p', 'ʌ', 'l', 'i', 's', 'm', 'ʌ', 'n', 'n', 'oʊ', 'z', 'ð', 'ʌ', 'w', 'eɪ']\n",
      "['ð', 'ʌ', 'p', 'ʌ', 'l', 'i', 's', 'm', 'æ', 'n', 'n', 'oʊ', 'z', 'ð', 'ʊ', 'w', 'eɪ']\n",
      "[1, 2, 6, 8, 11, 13, 18, 23, 25, 31, 34, 36, 42, 46, 47, 51, 54]\n",
      "[1, 2, 6, 8, 11, 13, 18, 23, 25, 31, 34, 36, 42, 46, 47, 51, 54]\n",
      "\n",
      "\n",
      "the buckets fill up quickly\n",
      "['DH', 'AH0', 'B', 'AH1', 'K', 'AH0', 'T', 'S', 'F', 'IH1', 'L', 'AH1', 'P', 'K', 'W', 'IH1', 'K', 'L', 'IY0']\n",
      "['DH', 'AH0', 'B', 'AH1', 'K', 'AH0', 'T', 'S', 'F', 'IH1', 'L', 'AH1', 'P', 'K', 'W', 'IH1', 'K', 'L', 'IY0']\n",
      "['ð', 'ʌ', 'b', 'ʌ', 'k', 'ʌ', 't', 's', 'f', 'ɪ', 'l', 'ʌ', 'p', 'k', 'w', 'ɪ', 'k', 'l', 'i']\n",
      "['ð', 'b', 'ʌ', 'k', 'ɪ', 't', 's', 'f', 'ɪ', 'l', 'ʌ', 'p', 'k', 'w', 'ɪ', 'k', 'l', 'i']\n",
      "[2, 8, 10, 16, 18, 22, 23, 29, 31, 35, 39, 43, 47, 50, 51, 56, 60, 62]\n",
      "[2, -1, 8, 10, 16, 18, 22, 23, 29, 31, 35, 39, 43, 47, 50, 51, 56, 60, 62]\n",
      "\n",
      "\n",
      "the boy is running away\n",
      "['DH', 'AH0', 'B', 'OY1', 'IH1', 'Z', 'R', 'AH1', 'N', 'IH0', 'NG', 'AH0', 'W', 'EY1']\n",
      "['DH', 'AH0', 'B', 'OY1', 'IH1', 'Z', 'R', 'AH1', 'N', 'IH0', 'NG', 'AH0', 'W', 'EY1']\n",
      "['ð', 'ʌ', 'b', 'ɔɪ', 'ɪ', 'z', 'ɹ', 'ʌ', 'n', 'ɪ', 'ŋ', 'ʌ', 'w', 'eɪ']\n",
      "['ð', 'ʌ', 'b', 'ɔɪ', 'ɪ', 'z', 'ɹ', 'ʌ', 'n', 'ɪ', 'ŋ', 'ʌ', 'w', 'eɪ']\n",
      "[1, 3, 7, 10, 22, 25, 28, 30, 34, 35, 38, 40, 44, 47]\n",
      "[1, 3, 7, 10, 22, 25, 28, 30, 34, 35, 38, 40, 44, 47]\n",
      "\n",
      "\n",
      "a towel is near the sink\n",
      "['AH0', 'T', 'AW1', 'L', 'IH1', 'Z', 'N', 'IH1', 'R', 'DH', 'AH0', 'S', 'IH1', 'NG', 'K']\n",
      "['AH0', 'T', 'AW1', 'AH0', 'L', 'IH1', 'Z', 'N', 'IH1', 'R', 'DH', 'AH0', 'S', 'IH1', 'NG', 'K']\n",
      "['ʌ', 't', 'aʊ', 'ʌ', 'l', 'ɪ', 'z', 'n', 'ɪ', 'ɹ', 'ð', 'ʌ', 's', 'ɪ', 'ŋ', 'k']\n",
      "['ʌ', 't', 'aʊ', 'l', 'ɪ', 'z', 'n', 'ɪ', 'ð', 'ɪ', 's', 'ɪ', 'ŋ', 'k']\n",
      "[2, 6, 11, 22, 27, 30, 34, 37, 44, 46, 50, 55, 61, 65]\n",
      "[2, 6, 11, -1, 22, 27, 30, 34, 37, -1, 44, 46, 50, 55, 61, 65]\n",
      "\n",
      "\n",
      "flowers can grow in the pot\n",
      "['F', 'L', 'AW1', 'ER0', 'Z', 'K', 'AH0', 'N', 'G', 'R', 'OW1', 'IH1', 'N', 'DH', 'AH0', 'P', 'AA1', 'T']\n",
      "['F', 'L', 'AW1', 'ER0', 'Z', 'K', 'AE1', 'N', 'G', 'R', 'OW1', 'IH0', 'N', 'DH', 'AH0', 'P', 'AA1', 'T']\n",
      "['f', 'l', 'aʊ', 'ɜ', 'z', 'k', 'æ', 'n', 'ɡ', 'ɹ', 'oʊ', 'ɪ', 'n', 'ð', 'ʌ', 'p', 'ɑ', 't']\n",
      "['l', 'aʊ', 'ɹ', 'z', 'k', 'æ', 'n', 'ɡ', 'ɹ', 'oʊ', 'ɪ', 'n', 'ð', 'ʌ', 'p', 'aʊ']\n",
      "[3, 5, 13, 19, 23, 25, 28, 32, 33, 36, 44, 46, 48, 49, 53, 57]\n",
      "[-1, 3, 5, 13, 19, 23, 25, 28, 32, 33, 36, 44, 46, 48, 49, 53, 57, -1]\n",
      "\n",
      "\n",
      "he's skating with his friend\n",
      "['HH', 'IY1', 'Z', 'S', 'K', 'EY1', 'T', 'AH0', 'N', 'W', 'IH1', 'TH', 'HH', 'AH0', 'Z', 'F', 'R', 'EH1', 'N', 'D']\n",
      "['HH', 'IY1', 'Z', 'S', 'K', 'EY1', 'T', 'IH0', 'NG', 'W', 'IH1', 'DH', 'HH', 'IH1', 'Z', 'F', 'R', 'EH1', 'N', 'D']\n",
      "['h', 'i', 'z', 's', 'k', 'eɪ', 't', 'ɪ', 'ŋ', 'w', 'ɪ', 'ð', 'h', 'ɪ', 'z', 'f', 'ɹ', 'e', 'n', 'd']\n",
      "['h', 'ɪ', 'z', 's', 'k', 'eɪ', 't', 'ɪ', 'ŋ', 'w', 'ɪ', 'ð', 'ɪ', 'z', 'f', 'ɹ', 'æ', 'n', 'd']\n",
      "[1, 3, 8, 9, 14, 16, 21, 23, 26, 28, 29, 32, 35, 39, 43, 45, 47, 54, 58]\n",
      "[1, 3, 8, 9, 14, 16, 21, 23, 26, 28, 29, 32, -1, 35, 39, 43, 45, 47, 54, 58]\n",
      "\n",
      "\n",
      "the janitor swept the floor\n",
      "['DH', 'AH0', 'JH', 'AE1', 'N', 'AH0', 'T', 'ER0', 'S', 'W', 'EH1', 'P', 'T', 'DH', 'AH0', 'F', 'L', 'AO1', 'R']\n",
      "['DH', 'AH0', 'JH', 'AE1', 'N', 'AH0', 'T', 'ER0', 'S', 'W', 'EH1', 'P', 'T', 'DH', 'AH0', 'F', 'L', 'AO1', 'R']\n",
      "['ð', 'ʌ', 'dʒ', 'æ', 'n', 'ʌ', 't', 'ɜ', 's', 'w', 'e', 'p', 't', 'ð', 'ʌ', 'f', 'l', 'ɒ', 'ɹ']\n",
      "['ð', 'ɪ', 'dʒ', 'æ', 'n', 'ɪ', 't', 'ɹ', 's', 'w', 'e', 'p', 't', 'ð', 'ʌ', 'f', 'l', 'oʊ']\n",
      "[2, 3, 8, 12, 17, 18, 22, 23, 29, 33, 35, 39, 41, 45, 46, 51, 54, 56]\n",
      "[2, 3, 8, 12, 17, 18, 22, 23, 29, 33, 35, 39, 41, 45, 46, 51, 54, 56, -1]\n",
      "\n",
      "\n",
      "the lady washed the shirt\n",
      "['DH', 'AH1', 'L', 'EY1', 'D', 'IY0', 'W', 'AA1', 'SH', 'T', 'DH', 'AH0', 'SH', 'ER1', 'T']\n",
      "['DH', 'AH0', 'L', 'EY1', 'D', 'IY0', 'W', 'AA1', 'SH', 'T', 'DH', 'AH0', 'SH', 'ER1', 'T']\n",
      "['ð', 'ʌ', 'l', 'eɪ', 'd', 'i', 'w', 'ɑ', 'ʃ', 't', 'ð', 'ʌ', 'ʃ', 'ɜ', 't']\n",
      "['ð', 'ʌ', 'l', 'eɪ', 'd', 'i', 'w', 'æ', 'ʃ', 't', 'ð', 'ɪ', 'ʃ', 'oʊ', 't']\n",
      "[1, 3, 7, 9, 15, 16, 22, 24, 33, 35, 40, 41, 46, 51, 62]\n",
      "[1, 3, 7, 9, 15, 16, 22, 24, 33, 35, 40, 41, 46, 51, 62]\n",
      "\n",
      "\n",
      "she took off her fur coat\n",
      "['SH', 'IY1', 'T', 'UH1', 'K', 'AO1', 'F', 'HH', 'ER1', 'F', 'ER1', 'K', 'OW1', 'T']\n",
      "['SH', 'IY1', 'T', 'UH1', 'K', 'AO1', 'F', 'HH', 'ER0', 'F', 'ER1', 'K', 'OW1', 'T']\n",
      "['ʃ', 'i', 't', 'ʊ', 'k', 'ɒ', 'f', 'h', 'ɜ', 'f', 'ɜ', 'k', 'oʊ', 't']\n",
      "['ʃ', 'i', 't', 'ʊ', 'k', 'aʊ', 'f', 'h', 'ɹ', 'f', 'ɹ', 'k', 'oʊ', 't']\n",
      "[2, 5, 10, 13, 18, 21, 28, 30, 31, 38, 41, 48, 52, 62]\n",
      "[2, 5, 10, 13, 18, 21, 28, 30, 31, 38, 41, 48, 52, 62]\n",
      "\n",
      "\n",
      "the match boxes are empty\n",
      "['DH', 'AH0', 'M', 'AE1', 'CH', 'B', 'AA1', 'K', 'S', 'AH0', 'Z', 'ER0', 'EH1', 'M', 'T', 'IY0']\n",
      "['DH', 'AH0', 'M', 'AE1', 'CH', 'B', 'AA1', 'K', 'S', 'AH0', 'Z', 'AA1', 'R', 'EH1', 'M', 'P', 'T', 'IY0']\n",
      "['ð', 'ʌ', 'm', 'æ', 'tʃ', 'b', 'ɑ', 'k', 's', 'ʌ', 'z', 'ɑ', 'ɹ', 'e', 'm', 'p', 't', 'i']\n",
      "['ð', 'm', 'æ', 'tʃ', 'b', 'aʊ', 'k', 's', 'ɪ', 'z', 'æ', 'ɹ', 'æ', 'm', 'p', 't', 'i']\n",
      "[2, 8, 11, 18, 22, 25, 31, 33, 36, 40, 43, 45, 49, 53, 55, 59, 61]\n",
      "[2, -1, 8, 11, 18, 22, 25, 31, 33, 36, 40, 43, 45, 49, 53, 55, 59, 61]\n",
      "\n",
      "\n",
      "the man is painting a sign\n",
      "['DH', 'AH0', 'M', 'AE1', 'N', 'IH1', 'Z', 'P', 'EY1', 'N', 'T', 'IH0', 'NG', 'AH0', 'S', 'AY1', 'N']\n",
      "['DH', 'AH0', 'M', 'AE1', 'N', 'IH1', 'Z', 'P', 'EY1', 'N', 'T', 'IH0', 'NG', 'AH0', 'S', 'AY1', 'N']\n",
      "['ð', 'ʌ', 'm', 'æ', 'n', 'ɪ', 'z', 'p', 'eɪ', 'n', 't', 'ɪ', 'ŋ', 'ʌ', 's', 'aɪ', 'n']\n",
      "['ð', 'm', 'æ', 'n', 'ɪ', 'z', 'p', 'eɪ', 'n', 't', 'ɪ', 'ŋ', 'ʌ', 's', 'aɪ', 'n']\n",
      "[1, 7, 10, 17, 21, 24, 28, 31, 35, 38, 41, 43, 46, 51, 56, 68]\n",
      "[1, -1, 7, 10, 17, 21, 24, 28, 31, 35, 38, 41, 43, 46, 51, 56, 68]\n",
      "\n",
      "\n",
      "the dog came home at last\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'K', 'EY1', 'M', 'HH', 'OW1', 'M', 'AE1', 'T', 'L', 'AE1', 'S', 'T']\n",
      "['DH', 'AH0', 'D', 'AO1', 'G', 'K', 'EY1', 'M', 'HH', 'OW1', 'M', 'AE1', 'T', 'L', 'AE1', 'S', 'T']\n",
      "['ð', 'ʌ', 'd', 'ɒ', 'ɡ', 'k', 'eɪ', 'm', 'h', 'oʊ', 'm', 'æ', 't', 'l', 'æ', 's', 't']\n",
      "['ð', 'd', 'aʊ', 'ɡ', 'k', 'eɪ', 'm', 'h', 'oʊ', 'm', 'æ', 't', 'l', 'æ', 's', 't']\n",
      "[1, 8, 11, 19, 23, 26, 30, 33, 36, 43, 45, 48, 52, 55, 68, 74]\n",
      "[1, -1, 8, 11, 19, 23, 26, 30, 33, 36, 43, 45, 48, 52, 55, 68, 74]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence =tg_sentence_mark[0] #\"This is a test sentence\"\n",
    "words = sentence.split()\n",
    "phonemes = [get_phonemes(word) for word in words]\n",
    "\n",
    "for _,i in enumerate(tg_sentence):\n",
    "    english_phonemes = ['<pad>', '<s>', '</s>', '<unk>','p', 'b', 't', 'd', 'k', 'ɡ','m', 'n', 'ŋ', 'f', 'v', 'θ', 'ð', 's', 'z', 'ʃ', 'h', 'tʃ', 'dʒ', 'l', 'ɹ', 'w', 'j',\"i\",\"ɪ\",\"ʊ\",\"u\",\"e\",\"ɜ\",\"æ\",\"ʌ\",\"ɑ\",\"ɒ\",\"eɪ\",\"ɔɪ\",\"oʊ\",\"aɪ\",\"aʊ\"]\n",
    "    \n",
    "    english_phoneme_dict = {k: v for k, v in processor_P.tokenizer.get_vocab().items() if k in english_phonemes}\n",
    "    if tg_sentence_mark[_]=='a boy fell from a window':\n",
    "        tg_sentence_mark[_]='a boy fell from the window'\n",
    "    stanford_phonemes1 = [j for i in [get_phonemes(word) for word in tg_sentence_mark[_].split()] for j in i[0]]\n",
    "    \n",
    "    stanford_phonemes2=[arpabet_to_ipa[j] for i in [get_phonemes(word) for word in tg_sentence_mark[_].split()] for j in i[0]]\n",
    "    each_phonemes =[i.mark for i in tg[-1] if tg_sentence[_].minTime<=i.minTime and tg_sentence[_].maxTime>=i.maxTime and i.mark!=\"\" and i.mark!=\"sp\" and i.mark!=\"sil\"]\n",
    "    \n",
    "    tg_word = [i for i in tg[1] if i.mark!=\"\" and i.mark!=\"sp\"]\n",
    "    wave, sr = librosa.load(ALL_ENG_ENG_pathset[0])\n",
    "    wave_res = librosa.resample(wave, orig_sr=sr, target_sr=16000)\n",
    "    \n",
    "    \n",
    "    input=processor(wave_res[int(tg_sentence[_].minTime*16000):round(tg_sentence[_].maxTime*16000)], sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
    "    model_P.to(device)\n",
    "    with torch.no_grad():\n",
    "        out_encoder=model_P(input.to(device)).logits\n",
    "    mask = np.ones(out_encoder.shape[-1], dtype=bool)\n",
    "    mask[list(english_phoneme_dict.values())] = False\n",
    "    out_encoder[:, :, mask] = 0\n",
    "    outind=torch.argmax(out_encoder,dim=-1).cpu().numpy()\n",
    "    phonemeindex = CTC_index(processor,outind)\n",
    "    transcription = processor_P.batch_decode(outind)[0].split(\" \")\n",
    "    aligned_seq1, aligned_seq2 ,phonemeindex_= align_sequences_with_index(stanford_phonemes2,transcription,phonemeindex)\n",
    "    \n",
    "    #if len(stanford_phonemes)!=len(each_phonemes):\n",
    "    print(tg_sentence_mark[_])\n",
    "    \n",
    "    print(each_phonemes)\n",
    "    print(stanford_phonemes1)\n",
    "    print(stanford_phonemes2)\n",
    "    print(transcription)\n",
    "    \n",
    "    print(phonemeindex)\n",
    "    print(phonemeindex_)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1=['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'R', 'AH1', 'M', 'HH', 'ER0', 'OW1', 'N', 'K', 'AH1', 'P'] \n",
    "seq2=['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'ER0', 'M', 'HH', 'ER1', 'OW1', 'N', 'K', 'AH1', 'P']\n",
    "a,b=align_sequences(seq2, seq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 2, 3, 4]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa=[0,1,1,2,2,3,4,3]\n",
    "del aa[-1]\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    <audio controls>\n",
       "                        <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\"/>\n",
       "                        Your browser does not support the audio element.\n",
       "                    </audio>\n",
       "                  "
      ],
      "text/plain": [
       "<pydub.audio_segment.AudioSegment at 0x2c28f7d0c40>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "count=18\n",
    "song = AudioSegment.from_wav(ALL_ENG_ENG_pathset[0])[tg_sentence[count].minTime*1000:tg_sentence[count].maxTime*1000]\n",
    "song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'ER0', '<pad>', 'M', 'HH', 'ER1', 'OW1', 'N', 'K', 'AH1', 'P']\n",
      "['SH', 'IY1', 'Z', 'D', 'R', 'IH1', 'NG', 'K', 'IH0', 'NG', 'F', 'R', 'AH1', 'M', 'HH', 'ER0', 'OW1', 'N', 'K', 'AH1', 'P']\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> 0\n",
      "<unk> 3\n",
      "<s> 1\n",
      "</s> 2\n",
      "ɑ 59\n",
      "ɑ 59\n",
      "ɑ 59\n",
      "æ 36\n",
      "æ 36\n",
      "æ 36\n",
      "ə 7\n",
      "ʌ 33\n",
      "ʌ 33\n",
      "ɔ 40\n",
      "ɔ 40\n",
      "ɔ 40\n",
      "aʊ 53\n",
      "aʊ 53\n",
      "aʊ 53\n",
      "aɪ 37\n",
      "aɪ 37\n",
      "aɪ 37\n",
      "b 26\n",
      "tʃ 66\n",
      "d 12\n",
      "ð 22\n",
      "ɛ 14\n",
      "ɛ 14\n",
      "ɛ 14\n",
      "ɚ 43\n",
      "ɚ 43\n",
      "ɚ 43\n",
      "eɪ 44\n",
      "eɪ 44\n",
      "eɪ 44\n",
      "f 23\n",
      "ɡ 35\n",
      "h 39\n",
      "ɪ 17\n",
      "ɪ 17\n",
      "ɪ 17\n",
      "i 10\n",
      "i 10\n",
      "i 10\n",
      "dʒ 60\n",
      "k 11\n",
      "l 8\n",
      "m 13\n",
      "n 4\n",
      "ŋ 42\n",
      "oʊ 49\n",
      "oʊ 49\n",
      "oʊ 49\n",
      "ɔɪ 100\n",
      "ɔɪ 100\n",
      "ɔɪ 100\n",
      "p 18\n",
      "ɹ 27\n",
      "s 5\n",
      "ʃ 38\n",
      "t 6\n",
      "θ 52\n",
      "ʊ 29\n",
      "ʊ 29\n",
      "ʊ 29\n",
      "u 34\n",
      "u 34\n",
      "u 34\n",
      "v 25\n",
      "w 32\n",
      "j 24\n",
      "z 21\n",
      "ʒ 65\n"
     ]
    }
   ],
   "source": [
    "arpabet_to_ipa={\n",
    "    '<pad>': '<pad>',\n",
    "    '<unk>': '<unk>',\n",
    "    '<s>': '<s>',\n",
    "    '</s>': '</s>',\n",
    "    'AA0': 'ɑ',\n",
    "    'AA1': 'ɑ',#ː\n",
    "    'AA2': 'ɑ',\n",
    "    'AE0': 'æ',\n",
    "    'AE1': 'æ',\n",
    "    'AE2': 'æ',\n",
    "    'AH0': 'ə',\n",
    "    'AH1': 'ʌ',\n",
    "    'AH2': 'ʌ',\n",
    "    'AO0': 'ɔ',\n",
    "    'AO1': 'ɔ',#ː\n",
    "    'AO2': 'ɔ',\n",
    "    'AW0': 'aʊ',\n",
    "    'AW1': 'aʊ',\n",
    "    'AW2': 'aʊ',\n",
    "    'AY0': 'aɪ',\n",
    "    'AY1': 'aɪ',\n",
    "    'AY2': 'aɪ',\n",
    "    'B': 'b',\n",
    "    'CH': 'tʃ',\n",
    "    'D': 'd',\n",
    "    'DH': 'ð',\n",
    "    'EH0': 'ɛ',\n",
    "    'EH1': 'ɛ',\n",
    "    'EH2': 'ɛ',\n",
    "    'ER0': 'ɚ',\n",
    "    'ER1': 'ɚ',\n",
    "    'ER2': 'ɚ',\n",
    "    'EY0': 'eɪ',\n",
    "    'EY1': 'eɪ',\n",
    "    'EY2': 'eɪ',\n",
    "    'F': 'f',\n",
    "    'G': 'ɡ',\n",
    "    'HH': 'h',\n",
    "    'IH0': 'ɪ',\n",
    "    'IH1': 'ɪ',\n",
    "    'IH2': 'ɪ',\n",
    "    'IY0': 'i',\n",
    "    'IY1': 'i',#ː\n",
    "    'IY2': 'i',\n",
    "    'JH': 'dʒ',\n",
    "    'K': 'k',\n",
    "    'L': 'l',\n",
    "    'M': 'm',\n",
    "    'N': 'n',\n",
    "    'NG': 'ŋ',\n",
    "    'OW0': 'oʊ',\n",
    "    'OW1': 'oʊ',\n",
    "    'OW2': 'oʊ',\n",
    "    'OY0': 'ɔɪ',\n",
    "    'OY1': 'ɔɪ',\n",
    "    'OY2': 'ɔɪ',\n",
    "    'P': 'p',\n",
    "    'R': 'ɹ',\n",
    "    'S': 's',\n",
    "    'SH': 'ʃ',\n",
    "    'T': 't',\n",
    "    'TH': 'θ',\n",
    "    'UH0': 'ʊ',\n",
    "    'UH1': 'ʊ',\n",
    "    'UH2': 'ʊ',\n",
    "#    'UW': 'uː',\n",
    "    'UW0': 'u',\n",
    "    'UW1': 'u',#ː\n",
    "    'UW2': 'u',\n",
    "    'V': 'v',\n",
    "    'W': 'w',\n",
    "    'Y': 'j',\n",
    "    'Z': 'z',\n",
    "    'ZH': 'ʒ',\n",
    "}\n",
    "for i in list(arpabet_to_ipa.values()):\n",
    "    print(i,processor_P.tokenizer.get_vocab()[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<s>': 1,\n",
       " '<pad>': 0,\n",
       " '</s>': 2,\n",
       " '<unk>': 3,\n",
       " 'n': 4,\n",
       " 's': 5,\n",
       " 't': 6,\n",
       " 'ə': 7,\n",
       " 'l': 8,\n",
       " 'a': 9,\n",
       " 'i': 10,\n",
       " 'k': 11,\n",
       " 'd': 12,\n",
       " 'm': 13,\n",
       " 'ɛ': 14,\n",
       " 'ɾ': 15,\n",
       " 'e': 16,\n",
       " 'ɪ': 17,\n",
       " 'p': 18,\n",
       " 'o': 19,\n",
       " 'ɐ': 20,\n",
       " 'z': 21,\n",
       " 'ð': 22,\n",
       " 'f': 23,\n",
       " 'j': 24,\n",
       " 'v': 25,\n",
       " 'b': 26,\n",
       " 'ɹ': 27,\n",
       " 'ʁ': 28,\n",
       " 'ʊ': 29,\n",
       " 'iː': 30,\n",
       " 'r': 31,\n",
       " 'w': 32,\n",
       " 'ʌ': 33,\n",
       " 'u': 34,\n",
       " 'ɡ': 35,\n",
       " 'æ': 36,\n",
       " 'aɪ': 37,\n",
       " 'ʃ': 38,\n",
       " 'h': 39,\n",
       " 'ɔ': 40,\n",
       " 'ɑː': 41,\n",
       " 'ŋ': 42,\n",
       " 'ɚ': 43,\n",
       " 'eɪ': 44,\n",
       " 'β': 45,\n",
       " 'uː': 46,\n",
       " 'y': 47,\n",
       " 'ɑ̃': 48,\n",
       " 'oʊ': 49,\n",
       " 'ᵻ': 50,\n",
       " 'eː': 51,\n",
       " 'θ': 52,\n",
       " 'aʊ': 53,\n",
       " 'ts': 54,\n",
       " 'oː': 55,\n",
       " 'ɔ̃': 56,\n",
       " 'ɣ': 57,\n",
       " 'ɜ': 58,\n",
       " 'ɑ': 59,\n",
       " 'dʒ': 60,\n",
       " 'əl': 61,\n",
       " 'x': 62,\n",
       " 'ɜː': 63,\n",
       " 'ç': 64,\n",
       " 'ʒ': 65,\n",
       " 'tʃ': 66,\n",
       " 'ɔː': 67,\n",
       " 'ɑːɹ': 68,\n",
       " 'ɛ̃': 69,\n",
       " 'ʎ': 70,\n",
       " 'ɔːɹ': 71,\n",
       " 'ʋ': 72,\n",
       " 'aː': 73,\n",
       " 'ɕ': 74,\n",
       " 'œ': 75,\n",
       " 'ø': 76,\n",
       " 'oːɹ': 77,\n",
       " 'ɲ': 78,\n",
       " 'yː': 79,\n",
       " 'ʔ': 80,\n",
       " 'iə': 81,\n",
       " 'i5': 82,\n",
       " 's.': 83,\n",
       " 'tɕ': 84,\n",
       " '??': 85,\n",
       " 'nʲ': 86,\n",
       " 'ɛː': 87,\n",
       " 'œ̃': 88,\n",
       " 'ɭ': 89,\n",
       " 'ɔø': 90,\n",
       " 'ʑ': 91,\n",
       " 'tʲ': 92,\n",
       " 'ɨ': 93,\n",
       " 'ɛɹ': 94,\n",
       " 'ts.': 95,\n",
       " 'rʲ': 96,\n",
       " 'ɪɹ': 97,\n",
       " 'ɭʲ': 98,\n",
       " 'i.5': 99,\n",
       " 'ɔɪ': 100,\n",
       " 'q': 101,\n",
       " 'sʲ': 102,\n",
       " 'u5': 103,\n",
       " 'ʊɹ': 104,\n",
       " 'iɜ': 105,\n",
       " 'a5': 106,\n",
       " 'iɛ5': 107,\n",
       " 'øː': 108,\n",
       " 'ʕ': 109,\n",
       " 'ja': 110,\n",
       " 'əɜ': 111,\n",
       " 'th': 112,\n",
       " 'ɑ5': 113,\n",
       " 'oɪ': 114,\n",
       " 'dʲ': 115,\n",
       " 'ə5': 116,\n",
       " 'tɕh': 117,\n",
       " 'ts.h': 118,\n",
       " 'mʲ': 119,\n",
       " 'ɯ': 120,\n",
       " 'dʑ': 121,\n",
       " 'vʲ': 122,\n",
       " 'e̞': 123,\n",
       " 'tʃʲ': 124,\n",
       " 'ei5': 125,\n",
       " 'o5': 126,\n",
       " 'onɡ5': 127,\n",
       " 'ɑu5': 128,\n",
       " 'iɑ5': 129,\n",
       " 'ai5': 130,\n",
       " 'aɪɚ': 131,\n",
       " 'kh': 132,\n",
       " 'ə1': 133,\n",
       " 'ʐ': 134,\n",
       " 'i2': 135,\n",
       " 'ʉ': 136,\n",
       " 'ħ': 137,\n",
       " 't[': 138,\n",
       " 'aɪə': 139,\n",
       " 'ʲ': 140,\n",
       " 'ju': 141,\n",
       " 'ə2': 142,\n",
       " 'u2': 143,\n",
       " 'oɜ': 144,\n",
       " 'pː': 145,\n",
       " 'iɛɜ': 146,\n",
       " 'ou5': 147,\n",
       " 'y5': 148,\n",
       " 'uɜ': 149,\n",
       " 'tː': 150,\n",
       " 'uo5': 151,\n",
       " 'd[': 152,\n",
       " 'uoɜ': 153,\n",
       " 'tsh': 154,\n",
       " 'ɑɜ': 155,\n",
       " 'ɵ': 156,\n",
       " 'i̪5': 157,\n",
       " 'uei5': 158,\n",
       " 'ɟ': 159,\n",
       " 'aɜ': 160,\n",
       " 'ɑɨ': 161,\n",
       " 'i.ɜ': 162,\n",
       " 'eʊ': 163,\n",
       " 'o2': 164,\n",
       " 'ɐ̃': 165,\n",
       " 'ä': 166,\n",
       " 'pʲ': 167,\n",
       " 'kʲ': 168,\n",
       " 'n̩': 169,\n",
       " 'ɒ': 170,\n",
       " 'ph': 171,\n",
       " 'ɑu2': 172,\n",
       " 'uɨ': 173,\n",
       " 'əɪ': 174,\n",
       " 'ɫ': 175,\n",
       " 'ɬ': 176,\n",
       " 'yɜ': 177,\n",
       " 'bʲ': 178,\n",
       " 'ɑ2': 179,\n",
       " 's̪': 180,\n",
       " 'aiɜ': 181,\n",
       " 'χ': 182,\n",
       " 'ɐ̃ʊ̃': 183,\n",
       " '1': 184,\n",
       " 'ə4': 185,\n",
       " 'yæɜ': 186,\n",
       " 'a2': 187,\n",
       " 'ɨː': 188,\n",
       " 't̪': 189,\n",
       " 'iouɜ': 190,\n",
       " 'ũ': 191,\n",
       " 'onɡɜ': 192,\n",
       " 'aɨ': 193,\n",
       " 'iɛ2': 194,\n",
       " 'ɔɨ': 195,\n",
       " 'ɑuɜ': 196,\n",
       " 'o̞': 197,\n",
       " 'ei2': 198,\n",
       " 'iou2': 199,\n",
       " 'c': 200,\n",
       " 'kː': 201,\n",
       " 'y2': 202,\n",
       " 'ɖ': 203,\n",
       " 'oe': 204,\n",
       " 'dˤ': 205,\n",
       " 'yɛɜ': 206,\n",
       " 'əʊ': 207,\n",
       " 'S': 208,\n",
       " 'ɡʲ': 209,\n",
       " 'onɡ2': 210,\n",
       " 'u\"': 211,\n",
       " 'eiɜ': 212,\n",
       " 'ʈ': 213,\n",
       " 'ɯᵝ': 214,\n",
       " 'iou5': 215,\n",
       " 'dZ': 216,\n",
       " 'r̝̊': 217,\n",
       " 'i.2': 218,\n",
       " 'tS': 219,\n",
       " 's^': 220,\n",
       " 'ʝ': 221,\n",
       " 'yə5': 222,\n",
       " 'iɑɜ': 223,\n",
       " 'uə5': 224,\n",
       " 'pf': 225,\n",
       " 'ɨu': 226,\n",
       " 'iɑ2': 227,\n",
       " 'ou2': 228,\n",
       " 'ər2': 229,\n",
       " 'fʲ': 230,\n",
       " 'ai2': 231,\n",
       " 'r̝': 232,\n",
       " 'uəɜ': 233,\n",
       " 'ɳ': 234,\n",
       " 'əɨ': 235,\n",
       " 'ua5': 236,\n",
       " 'uɪ': 237,\n",
       " 'ɽ': 238,\n",
       " 'bː': 239,\n",
       " 'yu5': 240,\n",
       " 'uo2': 241,\n",
       " 'yɛ5': 242,\n",
       " 'l̩': 243,\n",
       " 'ɻ': 244,\n",
       " 'ərɜ': 245,\n",
       " 'ʂ': 246,\n",
       " 'i̪2': 247,\n",
       " 'ouɜ': 248,\n",
       " 'uaɜ': 249,\n",
       " 'a.': 250,\n",
       " 'a.ː': 251,\n",
       " 'yæ5': 252,\n",
       " 'dː': 253,\n",
       " 'r̩': 254,\n",
       " 'ee': 255,\n",
       " 'ɪu': 256,\n",
       " 'ər5': 257,\n",
       " 'i̪ɜ': 258,\n",
       " 'æi': 259,\n",
       " 'u:': 260,\n",
       " 'i.ː': 261,\n",
       " 't^': 262,\n",
       " 'o1': 263,\n",
       " 'ɪ^': 264,\n",
       " 'ai': 265,\n",
       " 'ueiɜ': 266,\n",
       " 'æː': 267,\n",
       " 'ɛɪ': 268,\n",
       " 'eə': 269,\n",
       " 'i.': 270,\n",
       " 'ɴ': 271,\n",
       " 'ie': 272,\n",
       " 'ua2': 273,\n",
       " 'ɑ1': 274,\n",
       " 'o4': 275,\n",
       " 'tʃː': 276,\n",
       " 'o:': 277,\n",
       " 'ɑ:': 278,\n",
       " 'u1': 279,\n",
       " 'N': 280,\n",
       " 'i̪1': 281,\n",
       " 'au': 282,\n",
       " 'yæ2': 283,\n",
       " 'u.': 284,\n",
       " 'qː': 285,\n",
       " 'yəɜ': 286,\n",
       " 'y:': 287,\n",
       " 'kʰ': 288,\n",
       " 'tʃʰ': 289,\n",
       " 'iʊ': 290,\n",
       " 'sx': 291,\n",
       " 'õ': 292,\n",
       " 'uo': 293,\n",
       " 'tʰ': 294,\n",
       " 'uai5': 295,\n",
       " 'bʰ': 296,\n",
       " 'u.ː': 297,\n",
       " 'uə2': 298,\n",
       " 'ʊə': 299,\n",
       " 'd^': 300,\n",
       " 's̪ː': 301,\n",
       " 'yiɜ': 302,\n",
       " 'dʰ': 303,\n",
       " 'r.': 304,\n",
       " 'oe:': 305,\n",
       " 'i1': 306,\n",
       " 'ɟː': 307,\n",
       " 'yu2': 308,\n",
       " 'nʲʲ': 309,\n",
       " 'i̪4': 310,\n",
       " 'uei2': 311,\n",
       " 'tsʲ': 312,\n",
       " 'ɸ': 313,\n",
       " 'ĩ': 314,\n",
       " 'ɑ4': 315,\n",
       " 't̪ː': 316,\n",
       " 'eɑ': 317,\n",
       " 'u4': 318,\n",
       " 'e:': 319,\n",
       " 'tsː': 320,\n",
       " 'ʈʰ': 321,\n",
       " 'ɡʰ': 322,\n",
       " 'ɯɯ': 323,\n",
       " 'dʒʲ': 324,\n",
       " 'ʂʲ': 325,\n",
       " 'X': 326,\n",
       " 'ɵː': 327,\n",
       " 'uaiɜ': 328,\n",
       " 'tɕʲ': 329,\n",
       " 'ã': 330,\n",
       " 't^ː': 331,\n",
       " 'ẽː': 332,\n",
       " 'yɛ2': 333,\n",
       " 'cː': 334,\n",
       " 'i.1': 335,\n",
       " 'ɛʊ': 336,\n",
       " 'dˤdˤ': 337,\n",
       " 'dʒː': 338,\n",
       " 'i4': 339,\n",
       " 'ɡː': 340,\n",
       " 'yi': 341,\n",
       " 'ɕʲ': 342,\n",
       " 'ɟʰ': 343,\n",
       " 'pʰ': 344,\n",
       " 'dʑʲ': 345,\n",
       " 'yuɜ': 346,\n",
       " 'ua1': 347,\n",
       " 'ua4': 348,\n",
       " 'æiː': 349,\n",
       " 'ɐɐ': 350,\n",
       " 'ui': 351,\n",
       " 'iou1': 352,\n",
       " 'ʊː': 353,\n",
       " 'a1': 354,\n",
       " 'iou4': 355,\n",
       " 'cʰ': 356,\n",
       " 'iɛ1': 357,\n",
       " 'yə2': 358,\n",
       " 'ɖʰ': 359,\n",
       " 'ẽ': 360,\n",
       " 'ʒʲ': 361,\n",
       " 'ää': 362,\n",
       " 'ər4': 363,\n",
       " 'iːː': 364,\n",
       " 'ɪː': 365,\n",
       " 'iɑ1': 366,\n",
       " 'ər1': 367,\n",
       " 'œː': 368,\n",
       " 'øi': 369,\n",
       " 'ɪuː': 370,\n",
       " 'cʰcʰ': 371,\n",
       " 'əː1': 372,\n",
       " 'iː1': 373,\n",
       " 'ũ': 374,\n",
       " 'kʰː': 375,\n",
       " 'o̞o̞': 376,\n",
       " 'xʲ': 377,\n",
       " 'ou1': 378,\n",
       " 'iɛ4': 379,\n",
       " 'e̞e̞': 380,\n",
       " 'y1': 381,\n",
       " 'dzː': 382,\n",
       " 'dʲʲ': 383,\n",
       " 'dʰː': 384,\n",
       " 'ɯᵝɯᵝ': 385,\n",
       " 'lː': 386,\n",
       " 'uo1': 387,\n",
       " 'i.4': 388,\n",
       " 'i:': 389,\n",
       " 'yɛ5ʲ': 390,\n",
       " 'a4': 391}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor_P.tokenizer.get_vocab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BayesPCN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
